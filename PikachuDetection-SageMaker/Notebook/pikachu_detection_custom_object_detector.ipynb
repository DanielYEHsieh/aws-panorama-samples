{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to create a Custom Object Detector using GluonCV and deploy it to Panorama\n",
    "\n",
    "**About this Notebook** :\n",
    "\n",
    "* Fine-tuning is commonly used approach to transfer previously trained model to a new dataset.\n",
    "* It is especially useful if the targeting new dataset is relatively small.\n",
    "* Finetuning from pre-trained models can help reduce the risk of overfitting.\n",
    "* Finetuned model may also generalize better if the previously used dataset is in the similar domain of the new dataset.\n",
    "* This tutorial opens up a good approach for fine-tuning object detection models provided by GluonCV.\n",
    "* This notebook also shows finetuning using both local training and Amazon SageMaker managed training.\n",
    "\n",
    "**Goal of this Notebook** :\n",
    "\n",
    "* Aid an Panorama developer in creating a custom Object Detector using GluonCV\n",
    "* Once the model is created, export the model parameters\n",
    "* Use the exported parameters to then deploy the model\n",
    "* Create a sample lambda to loop in the model\n",
    "\n",
    "**What this Notebook accomplishes?** :\n",
    "\n",
    "* Show how to use a customized Pikachu dataset and illustrate the finetuning fundamentals step by step.\n",
    "* Walk thru the steps to modify a model to fit your own object detection projects.\n",
    "* This is an adaption of the following GluonCV example : [Link](https://gluon-cv.mxnet.io/build/examples_detection/finetune_detection.html)\n",
    "\n",
    "**Useful Resources to aid your development**:\n",
    "* [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)\n",
    "* [Create Your Own COCO Dataset](https://gluon-cv.mxnet.io/build/examples_datasets/mscoco.html#sphx-glr-build-examples-datasets-mscoco-py)\n",
    "* [Create Your Own VOC Dataset](https://gluon-cv.mxnet.io/build/examples_datasets/pascal_voc.html#sphx-glr-build-examples-datasets-pascal-voc-py)\n",
    "* [sphx-glr-build-examples-datasets-detection-custom](https://gluon-cv.mxnet.io/build/examples_datasets/detection_custom.html#sphx-glr-build-examples-datasets-detection-custom-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & config\n",
    "----------------\n",
    "This notebook was tested with ***GluonCV 0.8.0***. If you are running this notebook on Amazon SageMaker Notebook Instance then use `conda_mxnet_p36` kernel (or similar latest version) which comes with MXNet pre-installed (but you may still need to install GluonCV. \n",
    "\n",
    "*Also install MXNet if not running on SageMaker Notebook Instance by using `pip install mxnet` and restart a kernel (`Kernel->Restart` in menu). This notebook was last tested with* ***MXNet 1.6.0***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluoncv==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import tarfile\n",
    "import json\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.io import DataBatch\n",
    "import gluoncv as gcv\n",
    "from gluoncv.utils import download, viz\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "import training.train\n",
    "\n",
    "print(f'Using MXNet {mx.__version__}')\n",
    "print(f'Using GluonCV {gcv.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable/constant value to be the full name of your bucket, for example \"aws-panorama-example-xyz\"\n",
    "BUCKET = 'aws-panorama-<you-bucket-name-suffix>'  # Bucket name must contain \"aws-panorama\"\n",
    "\n",
    "MODELS_S3_PREFIX = 'models'\n",
    "models_s3_url = f's3://{BUCKET}/{MODELS_S3_PREFIX}'\n",
    "\n",
    "DATA_S3_PREFIX = 'data/pikachu'\n",
    "\n",
    "LAMBDA = 'PikachuDetection'\n",
    "LAMBDA_DIR = '../Lambda'\n",
    "\n",
    "LAMBDA_EXECUTION_ROLE_NAME = 'PanoramaPikachuLambdaExecutionRole'\n",
    "lambda_file = f'{LAMBDA}.py'\n",
    "lambda_archive = lambda_file.replace('.py', '.zip')\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "classes = training.train.CLASSES\n",
    "input_size = training.train.INPUT_SIZE\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "DATA_REC_FILES = ['train.rec']\n",
    "DATA_BASE_URL = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu'\n",
    "\n",
    "TRAIN_DIR = 'training'\n",
    "TRAIN_FILE = 'train.py'\n",
    "train_file_path = os.path.join(TRAIN_DIR, TRAIN_FILE)\n",
    "\n",
    "ctx = training.train.get_ctx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sagemaker.session.Session()\n",
    "region = ss.boto_region_name\n",
    "sm = boto3.client('sagemaker', region_name=region)\n",
    "s3 = boto3.client('s3')\n",
    "iam = boto3.client(\"iam\")\n",
    "lm = boto3.client(\"lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_code(func):\n",
    "    code = inspect.getsource(func)\n",
    "    formatter = HtmlFormatter(cssclass='pygments')\n",
    "    html_code = highlight(inspect.getsource(training.train.get_net), PythonLexer(), formatter)\n",
    "    css = formatter.get_style_defs('.pygments')\n",
    "    template = '<style>{}</style>{}'\n",
    "    html = template.format(css, html_code)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pikachu Dataset\n",
    "----------------\n",
    "Starting with a Pikachu dataset generated by rendering 3D models on random real-world scenes.\n",
    "\n",
    "Please refer to `sphx_glr_build_examples_datasets_detection_custom` link above for tutorial of how to create your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "for f in [rf.replace('.rec', ext) for rf in DATA_REC_FILES for ext in ('.rec', '.idx')]:\n",
    "    download(os.path.join(DATA_BASE_URL, f), path=os.path.join(DATA_DIR, f), overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset and show some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = gcv.data.RecordFileDetection(os.path.join(DATA_DIR, DATA_REC_FILES[0]))\n",
    "samples = [0, 1]\n",
    "_, ax = plt.subplots(len(samples), figsize=(15, 15))\n",
    "for i in [0, 1]:\n",
    "    image, label = dataset[i]\n",
    "    viz.plot_bbox(image, bboxes=label[:, :4], labels=label[:, 4:5], class_names=classes, ax=ax[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training code\n",
    "-------------------\n",
    "\n",
    "***This notebook shows two ways of training the model - training locally and training on Amazon SageMaker. Both examples use the same training code, which can be found in `training/train.py` and is not replicated in this notebook***\n",
    "\n",
    "Here is the full training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $train_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-trained models\n",
    "-------------------\n",
    "\n",
    "Instead of building and training a model from scratch, let's build one by finetuning a pre-trained model. There are multiple choices in the [GluonCV Model Zoo](https://cv.gluon.ai/model_zoo/detection.html). A fast SSD network with MobileNet1.0 backbone was selected for this sample.\n",
    "\n",
    "This is the part of the training code responsible for loading and configuring the pre-trained model to work with classes we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code(training.train.get_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's load our chosed model and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_net = training.train.get_net(ctx=ctx)\n",
    "local_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is also an alternative way for creating custom network with pre-trained weights, shown here for informational purposes only*\n",
    "\n",
    "If you want to try this way, convert the next cell to Code type and run it to obtain the same model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_model = training.train.MODEL.replace('.0_custom', '.0_voc')\n",
    "print(f'Loading model {source_model} from Model Zoo')\n",
    "local_net = gcv.model_zoo.get_model(source_model, pretrained=True, ctx=ctx)\n",
    "local_net.reset_class(classes)\n",
    "local_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local training\n",
    "------------------\n",
    "\n",
    "Code in the next cell will train the model using a small number of epochs, which should only take few minutes to complete (for example, training for 2 epochs on a `p2.xlarge` instance can take about 5 minutes). You may try increasing the number of epochs to see if you obtain a better model (or drop it to as low as 1 if you just want to see how it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "local_model_dir = 'model'\n",
    "saved_params_file = 'saved-model.params'\n",
    "if os.path.exists(local_model_dir):\n",
    "    shutil.rmtree(local_model_dir)\n",
    "\n",
    "# Data and model paths should be relative to script running the training\n",
    "!cd $TRAIN_DIR && python3 $TRAIN_FILE --epochs $epochs --train ../$DATA_DIR \\\n",
    "    --model-dir ../$local_model_dir --save-params --saved-params-file $saved_params_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the fine-tuned model\n",
    "----------------------------\n",
    "Use an image not seen by the training process for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_source_url = 'https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/pikachu.jpg'\n",
    "def prepare_test_image(test_image_url=test_image_source_url, input_size=input_size, ctx=None):\n",
    "    test_image_name = os.path.split(test_image_url)[1]\n",
    "    if not os.path.exists(test_image_name):\n",
    "        print(f'Downloadig {test_image_url}')\n",
    "        download(test_image_url, test_image_name)\n",
    "    img, image = gcv.data.transforms.presets.ssd.load_test(test_image_name, input_size)\n",
    "    if ctx is not None:\n",
    "        img = img.as_in_context(ctx)\n",
    "    return img, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, image = prepare_test_image(input_size=input_size, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(cids, scores, bboxes, image, classes, thresh=THRESHOLD):\n",
    "    _, ax = plt.subplots(2, figsize=(15, 15))\n",
    "    ax[0].imshow(image)\n",
    "    viz.plot_bbox(image, bboxes[0], scores[0], cids[0], class_names=classes, thresh=thresh, ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local_net.load_parameters(os.path.join(local_model_dir, saved_params_file), ctx=ctx)\n",
    "show(*local_net(img), image, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker training\n",
    "------------------\n",
    "\n",
    "You may skip this section if all you want is to see how the model works on Panorama device as you already have a trainded model to deploy. However, following this section will give you a good overview of training the same model using SageMaker managed training.\n",
    "\n",
    "SageMaker managed training takes place outside of the instance running this notebook and needs access to training data. Storing the training data in S3 bucket is convenient way of achieving this due to easy integration of Amazon S3 with other AWS services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload training data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_inputs = ss.upload_data(bucket=BUCKET, path=DATA_DIR, key_prefix=DATA_S3_PREFIX)\n",
    "print(f'Uploaded training data to {s3_inputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch a training job\n",
    "\n",
    "SageMaker SDK provides resources to facilitate the training (and inference) using different Machine Learning frameworks, including the ability to use your own customer Docker containers for training and inference. The model used here is based on MXNet and there is a dedicated `MXNet` class in SageMaker SDK for training MXNet models using custom training code, exactly what we need.\n",
    "\n",
    "Running the next cell may take a bit longer than the local training because Amazon SageMaker needs to launch additional compute instances to perform the training (*and you only paying for the actual training time, not the time it takes to launch those instances*), however in return you get extra benefits like a flexibility of choosing different framework versions and instance types to use for training (i.e. you could be running this notebook on a small CPU based instance to minimise the development costs and use powerful GPU instances only for training) or ability to run multiple training jobs in the background while you keep on developing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch a training job\n",
    "\n",
    "Change the `wait` variable value to False if you want to carry on with other tasks while the training job is running at the background. Note that we are using a specific version of MXNet to run the training job which is actually different from the version we used for local training.\n",
    "\n",
    "*You will not be able to see the logs here if you run the training job in non-waiting mode, you can still find logs in Amazon CloudWatch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = True\n",
    "estimator = MXNet(TRAIN_FILE,\n",
    "                  source_dir=TRAIN_DIR,\n",
    "                  role=sagemaker.get_execution_role(),\n",
    "                  instance_count=1,\n",
    "                  instance_type=\"ml.p2.xlarge\",\n",
    "                  framework_version=\"1.7.0\",\n",
    "                  py_version=\"py3\",\n",
    "                  output_path=models_s3_url,\n",
    "                  hyperparameters={'epochs': 2})\n",
    "estimator.fit(s3_inputs, wait=wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you started the training job in non-waiting mode (`wait = False`), you can monitor the job progress in the Amazon SageMaker Console and see the logs in Amazon CloudWatch. Alternatively you can start a waiter which will block the notebook execution until the training stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter('training_job_completed_or_stopped')\n",
    "waiter.wait(TrainingJobName=estimator._current_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download and unpack the model artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'sm_model'\n",
    "job_info = sm.describe_training_job(TrainingJobName=estimator._current_job_name)\n",
    "job_status = job_info['TrainingJobStatus']\n",
    "print(f\"Job status: {job_status}\")\n",
    "sm_training_ok = job_status.upper() == 'COMPLETED'\n",
    "if sm_training_ok:\n",
    "    sm_model_s3_url = job_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "    print(f'Model S3 location: {sm_model_s3_url}')\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    os.mkdir(model_dir)\n",
    "    model_key = sm_model_s3_url[len(f's3://{BUCKET}/'):]\n",
    "    model_archive_file = os.path.split(sm_model_s3_url)[1]\n",
    "    print(f'Downloading model to {model_dir}')\n",
    "    s3.download_file(Bucket=BUCKET, Key=model_key, Filename=os.path.join(model_dir, model_archive_file))\n",
    "    !ls $model_dir\n",
    "    print(f'Extracing model artifacts')\n",
    "    !cd $model_dir && tar xvf $model_archive_file\n",
    "else:\n",
    "    job_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference\n",
    "\n",
    "This shows how to load an MXNet model without direct access to the model's definition class as we did earlier when we loaded saved parameters into the existing instance of the model class (`local_net`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net_from_checkpoint(checkpoint_prefix, ctx):\n",
    "    \"\"\"Load the model weights from checkpoint (e.g. created by `net.export`) as opposed to loading parameters\n",
    "    from file created by `net.save_paramerters`\n",
    "    https://github.com/awslabs/multi-model-server/tree/master/examples/ssd\n",
    "    \"\"\"\n",
    "    print(f'Loading model from {checkpoint_prefix}')\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix=checkpoint_prefix, epoch=0)\n",
    "\n",
    "    # We use the data_names and data_shapes returned by save_mxnet_model API.\n",
    "    mod = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\n",
    "    print(f'Model input name: {mod.data_names}')\n",
    "    mod.bind(data_shapes=[(mod.data_names[0], (1, 3, input_size, input_size))])\n",
    "    mod.set_params(arg_params, aux_params)\n",
    "    print(f'Model input shapes: {mod.data_shapes}')\n",
    "    print(f'Model output shapes: {mod.output_shapes}')\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_net = get_net_from_checkpoint(os.path.join(model_dir, 'exported-model'), training.train.get_ctx())\n",
    "img, image = prepare_test_image(ctx=ctx)\n",
    "sm_net.forward(DataBatch([img]))\n",
    "sm_output = sm_net.get_outputs()\n",
    "show(*sm_output, image, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model\n",
    "-----------------------\n",
    "\n",
    "There is nothing to do here if you want to use the model trained in Amazon SageMaker, the model is already in S3 and the exact location is in `sm_model_s3_url` variable.\n",
    "\n",
    "Take a note of the model's S3 location, you will need it during the application deployment to Panorama appliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here is a quick way to check the file on S3 if you have AWS CLI tools installed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model trained in SageMaker stored in {sm_model_s3_url}')\n",
    "!aws s3 ls $sm_model_s3_url --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a locally trained model then convert the following cells to Code type and run to pack the locally trained model and upload it to S3 bucket."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_archive = 'mxnet-pikachu.tar.gz'\n",
    "!cd $local_model_dir && tar -czvf ../$model_archive exported-model*.*\n",
    "\n",
    "model_key = os.path.join(MODELS_S3_PREFIX, model_archive)\n",
    "s3.upload_file(model_archive, Bucket=BUCKET, Key=model_key)\n",
    "\n",
    "local_model_s3_url = os.path.join(f's3://{BUCKET}', model_key)\n",
    "print(f'Locally trained model stored in {local_model_s3_url}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!aws s3 ls $local_model_s3_url --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### Upload, Create and Publish Lambda Function\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet uses boto3 to create an IAM role named LambdaBasicExecution with basic \n",
    "lambda execution permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam.create_role(\n",
    "    RoleName=LAMBDA_EXECUTION_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python snippet will use the resources above to create a new AWS Lambda function called PikachuDetection_demo. If you already have a Lambda Function with that name and want to re-create it, run the following cell after converting it to Code type."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lm.delete_function(FunctionName=LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm $LAMBDA_DIR/$lambda_archive\n",
    "!cd $LAMBDA_DIR && zip -o $lambda_archive $lambda_file\n",
    "!cp $LAMBDA_DIR/$lambda_archive .\n",
    "\n",
    "with open(os.path.join(LAMBDA_DIR, lambda_archive), \"rb\") as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "lambda_execution_role = iam.get_role(RoleName=LAMBDA_EXECUTION_ROLE_NAME)\n",
    "response = lm.create_function(\n",
    "    FunctionName=LAMBDA,\n",
    "    Runtime=\"python3.7\",\n",
    "    Role=lambda_execution_role[\"Role\"][\"Arn\"],\n",
    "    Handler=lambda_file.replace('.py', '.main()'),\n",
    "    Code=dict(ZipFile=zipped_code),\n",
    "    Timeout=120,  \n",
    "    MemorySize=2048,\n",
    "    Publish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the details of the lambda function that was just published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_arn = response[\"FunctionArn\"]\n",
    "function_arn_version = list(response[\"FunctionArn\"].split(\":\"))[-1]\n",
    "lambda_url = (\n",
    "    \"https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/\"\n",
    "    + response[\"FunctionName\"]\n",
    "    + \"/versions/\"\n",
    "    + response[\"Version\"]\n",
    "    + \"?tab=configuration\"\n",
    ")\n",
    "print(lambda_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### Next steps\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lambda is now created and published. You are now ready to deploy your model and the published lambda function, to the Panorama device\n",
    "\n",
    "The instructions to deploy are linked below\n",
    "\n",
    "[Creating Application Instructions Here](https://docs.aws.amazon.com/panorama/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
