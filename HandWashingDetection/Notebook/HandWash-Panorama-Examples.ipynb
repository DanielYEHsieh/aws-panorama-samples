{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to create a Hand Wash Detection And Timer Application using MXNet / PanoramaSDK\n",
    "\n",
    "**Goal of this Notebook** :\n",
    "\n",
    "* Aid an Panorama developer prototype their application before creating the AWS Lambda for Panorama\n",
    "* Creating an MXNET application in the same structure as the AWS Lambda for Panorama\n",
    "* Create and Deploy the AWS Lambda for Panorama from this notebook\n",
    "\n",
    "**What this Notebook accomplishes?** :\n",
    "* Detect Handwashing using Kinetics 400 Model using MXNet\n",
    "* Time the Handwashing and display the Handwashing time on screen\n",
    "* Show the structure of the AWS Lambda for Panorama and replicate it using MXNet\n",
    "* Create and publish the included AWS Lambda for Panorama directly to the AWS Lambda service\n",
    "\n",
    "\n",
    "**Useful Resources to aid your development**:\n",
    "* [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)\n",
    "\n",
    "\n",
    "**Pre -Requisites**:\n",
    "* Sagemaker Instance created with the right role (Policies needed IOT, Lambda and S3, IAM Full Access to your sagemaker role)\n",
    "\n",
    "\n",
    "**Frames to Process**:\n",
    "\n",
    "* By default, we only process 10 frames from the video. If you want to increase this, please change this value in /panorama_sdk/panoramasdk.py and change frames_to_process = 10 to a value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_use = \"washing_hands.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, image\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv import utils\n",
    "from gluoncv.model_zoo import get_model\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    \"\"\"\n",
    "    Helper Function for Fomatting Output\n",
    "    \"\"\"\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Modelling Approach\n",
    "\n",
    "This step walks through using the MXNet model to get inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model** : resnet101_v1b_kinetics400  \n",
    "* **Kinetics400** : These models are trained on [Kinetics400](https://deepmind.com/research/open-source/kinetics), a large-scale, high-quality dataset of URL links to approximately 650,000 video clips that covers 400 human action classes, including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging.   \n",
    "* **arXiv** :[The Kinetics Human Action Video Dataset](https://arxiv.org/abs/1705.06950)  \n",
    "* **Model Input Size** : 224 x 224  \n",
    "* **Model Output** : 400 Classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **A. Loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_detection_model_name = 'resnet101_v1b_kinetics400'\n",
    "action_detection_model = get_model(action_detection_model_name, nclass=400, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **B. Loading the input video**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = '../panorama_sdk/Sources/washing_hands.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **C. Preprocessing function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size):\n",
    "\n",
    "    \"\"\"\n",
    "    # Performing transformations for the video clip / camera feed. This transformation function\n",
    "    # does three things: center crop the image to 224x224 in size,\n",
    "    # transpose it and normalize with\n",
    "    # mean and standard deviation calculated across all frames.\n",
    "\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(img, (size, size))\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]  # RGB\n",
    "    std = [0.229, 0.224, 0.225]  # RGB\n",
    "\n",
    "    # converting array of ints to floats\n",
    "    img = resized.astype(np.float32) / 255.0\n",
    "    img_a = img[:, :, 0]\n",
    "    img_b = img[:, :, 1]\n",
    "    img_c = img[:, :, 2]\n",
    "\n",
    "    # Extracting single channels from 3 channel image\n",
    "    # normalizing per channel data:\n",
    "    img_a = (img_a - mean[0]) / std[0]\n",
    "    img_b = (img_b - mean[1]) / std[1]\n",
    "    img_c = (img_c - mean[2]) / std[2]\n",
    "\n",
    "    # putting the 3 channels back together:\n",
    "    x1 = [[[], [], []]]\n",
    "    x1[0][0] = img_a\n",
    "    x1[0][1] = img_b\n",
    "    x1[0][2] = img_c\n",
    "\n",
    "    x1 = np.asarray(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **D. Using the model to do inference**\n",
    "\n",
    "The below example code does inference only on the first 150 frames. The code also visualizes the top 3 actions detected at each frame\n",
    "  \n",
    "**Note :** The actual / meaningful predictions start at around Frame 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the video using opencv\n",
    "\n",
    "cap = cv2.VideoCapture(inputs)\n",
    "frame_number = 0\n",
    "inference_on_number_of_frames = 10\n",
    "\n",
    "# Looping thru the video\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    # Increment Frame Number\n",
    "    frame_number += 1\n",
    "    \n",
    "    (grabbed, frame) = cap.read()\n",
    "\n",
    "    # Preprocess Frame. We resize to 224 x 224\n",
    "    prep_frame = preprocess(frame, 224)\n",
    "\n",
    "    # Predict\n",
    "    pred = action_detection_model(nd.array(prep_frame))\n",
    "    \n",
    "    # Get Classes\n",
    "    classes = action_detection_model.classes\n",
    "    \n",
    "    # If you want to look at the classes, uncomment the next print statement. \n",
    "    # Warning: really large list will be printed. \n",
    "    \n",
    "    # Get Top n classes predicted\n",
    "    topK = 3\n",
    "    ind = nd.topk(pred, k=topK)[0].astype('int')\n",
    "    topKlist = []\n",
    "\n",
    "    # Collect the top n Classes Predicted and print on screen\n",
    "    for i in range(topK):\n",
    "        a = 25 + 25 * i\n",
    "        class_pred = classes[ind[i].asscalar()]\n",
    "        cv2.putText(frame, 'Action {}: {}'.format(i + 1,class_pred), (10, a), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, 'frame : {}'.format(frame_number), (10, a + 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Output the frame\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    \n",
    "    # Clear output\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # observe the keypress by the user\n",
    "    keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the user pressed \"q\", then stop looping\n",
    "    if keypress == ord(\"q\") or frame_number == inference_on_number_of_frames:\n",
    "        break\n",
    "\n",
    "# free up memory\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Understanding and creating the Structure of the Application\n",
    "\n",
    "* A, B, C and D within Step 2 showed how to use the Action Recognition model to do inference. \n",
    "\n",
    "* This step will introduce the structure of the Panorama Lambda. \n",
    "\n",
    "* This step also walks through the code to understand how the Lambda structure can be created. \n",
    "\n",
    "**Note**: \n",
    "\n",
    "* The code structure below is as close to the lambda structure as possible, but is **NOT** the actual Lambda.\n",
    "\n",
    "* To see and compare the actual lambda to this code, please open the HandWashDetection.py file in the Lambda folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Panorama Lambda function has the following structure\n",
    "\n",
    "**Note** : Actual Lambda inherits base class called panoramasdk.base instead of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lambda skeleton\n",
    "\n",
    "\n",
    "class HandWashDetection(object):\n",
    "    def interface(self):\n",
    "        # defines the parameters that interface with other services from Panorama\n",
    "        return\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        # defines the attributes such as arrays and model objects that will be used in the application\n",
    "        return\n",
    "\n",
    "    def entry(self, inputs, outputs):\n",
    "        # defines the application logic responsible for predicting using the inputs and handles what to do\n",
    "        # with the outputs\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above structure to create a class that resembles the Panorama lambda as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "import jupyter_utils\n",
    "\n",
    "jupyter_utils.change_video_source(video_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import boto3\n",
    "from classes import load_classes\n",
    "import json\n",
    "\n",
    "# [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "iot = boto3.client('iot-data', region_name='us-east-1')\n",
    "\n",
    "class HandWashDetection(panoramasdk.base):\n",
    "    \n",
    "    def interface(self):\n",
    "        return {\n",
    "            \"parameters\":\n",
    "            (\n",
    "                (\"model\", \"action_detection\", \"Model for detecting_action\", \"resnet101_v1b_kinetics400\"),\n",
    "            ),\n",
    "            \"inputs\":\n",
    "            (\n",
    "                (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "            ),\n",
    "            \"outputs\":\n",
    "            (\n",
    "                (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "            )\n",
    "            \n",
    "        }\n",
    "        \n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        \n",
    "        try:\n",
    "            # Frame Number\n",
    "            self.frame_num = 0\n",
    "\n",
    "            # Load model from the specified directory\n",
    "            print(\"loading model\")\n",
    "            self.action_detection_model = panoramasdk.model()\n",
    "            self.action_detection_model.open(parameters.action_detection, 1)\n",
    "            print(\"model loaded\")\n",
    "\n",
    "            # Panorama SDK specific declarations\n",
    "            self.class_name_list = []\n",
    "            self.class_prob_list = []\n",
    "            class_info = self.action_detection_model.get_output(0)\n",
    "            self.class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "                        \n",
    "            # Set up Timer\n",
    "            self.time_start = time.time()\n",
    "            self.seconds_to_wash = 120\n",
    "            \n",
    "            # Misc\n",
    "            self.list_actions = []\n",
    "            self.message = ''\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            return False\n",
    "    \n",
    "    def preprocess(self, img, size):\n",
    "        \n",
    "        resized = cv2.resize(img, (size, size))\n",
    "        mean = [0.485, 0.456, 0.406]  # RGB\n",
    "        std = [0.229, 0.224, 0.225]  # RGB\n",
    "        \n",
    "        img = resized.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "        img_a = img[:, :, 0]\n",
    "        img_b = img[:, :, 1]\n",
    "        img_c = img[:, :, 2]\n",
    "        \n",
    "        # Extracting single channels from 3 channel image\n",
    "        # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "        # normalizing per channel data:\n",
    "        img_a = (img_a - mean[0]) / std[0]\n",
    "        img_b = (img_b - mean[1]) / std[1]\n",
    "        img_c = (img_c - mean[2]) / std[2]\n",
    "        \n",
    "        # putting the 3 channels back together:\n",
    "        x1 = [[[], [], []]]\n",
    "        x1[0][0] = img_a\n",
    "        x1[0][1] = img_b\n",
    "        x1[0][2] = img_c\n",
    "        x1 = np.asarray(x1)\n",
    "        \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "    def entry(self, inputs, outputs):\n",
    "        \n",
    "        for i in range(len(inputs.video_in)):\n",
    "            stream = inputs.video_in[i]\n",
    "            \n",
    "            self.frame_num += 1\n",
    "            \n",
    "            image_input = stream.image\n",
    "            \n",
    "            # Pre Process Frame\n",
    "            prep_frame = self.preprocess(image_input, 224)\n",
    "\n",
    "            # Predict\n",
    "            self.action_detection_model.batch(0, prep_frame)\n",
    "            self.action_detection_model.flush()\n",
    "\n",
    "            # Get the results.\n",
    "            resultBatchSet = self.action_detection_model.get_result()\n",
    "                        \n",
    "            class_batch = resultBatchSet.get(0)\n",
    "            class_batch.get(0, self.class_array)\n",
    "            class_data = self.class_array[0]\n",
    "            \n",
    "            # Load Classes\n",
    "            classes = load_classes()\n",
    "            \n",
    "            # declare topKlist\n",
    "            topKlist = []\n",
    "            \n",
    "            # Collect the Top 10 Classes\n",
    "            sorted_vals = sorted(((value,index) for index, value in enumerate(class_data)), reverse=True)\n",
    "            ind = [d for (c,d) in sorted_vals][0:10]\n",
    "            for z in range(len(ind)):\n",
    "                class_name = classes[ind[z]]\n",
    "                topKlist.append(class_name)\n",
    "                \n",
    "                                    \n",
    "            # Here we want to be certain that we are detecting washing hands\n",
    "            if 'washing_hands' in topKlist:\n",
    "                self.list_actions.append(1)\n",
    "            else:\n",
    "                self.list_actions.append(0)\n",
    "            \n",
    "            if len(self.list_actions) > 20:\n",
    "                self.list_actions = self.list_actions[-20:]\n",
    "                    \n",
    "            # Once we definitively detect washing hands, we start writing to the frame\n",
    "            if ('washing_hands' in topKlist and sum(self.list_actions) > 10):\n",
    "                hours, rem = divmod(time.time() - self.time_start, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                count_seconds = self.seconds_to_wash - int(minutes*60 + seconds)\n",
    "                \n",
    "                # write on frame \n",
    "                #cv2.putText(image_input, 'Action: Washing Hands', (100, 25), cv2.FONT_HERSHEY_SIMPLEX , 0.5, (255, 0, 0) , 2, cv2.LINE_AA)\n",
    "                stream.add_label('Action: Washing Hands',0.1,0.25)\n",
    "                \n",
    "                if count_seconds <= 0:\n",
    "                    stream.add_label('Time : {}'.format(0),0.1,0.5)\n",
    "                    stream.add_label('Message : {}'.format('Great Job'),0.1,0.75)\n",
    "                    self.message = 'Great Job'\n",
    "                    \n",
    "                # Reset Time start after 10 seconds. Until then show message\n",
    "                if count_seconds <= -10:\n",
    "                    stream.add_label('Time : {}'.format(0),0.1,0.5)\n",
    "                    self.time_start = time.time()\n",
    "                    self.message = ''\n",
    "                else:\n",
    "                    \n",
    "                    stream.add_label('Time : {}'.format(count_seconds),0.1,0.5)\n",
    "                    self.message = ''\n",
    "            \n",
    "            # Top k action list to MQTT message\n",
    "            response = iot.publish(topic='HandWashing',qos=1,payload=json.dumps({\"results\":topKlist,\"message\":self.message}))\n",
    "        \n",
    "            self.action_detection_model.release_result(resultBatchSet)\n",
    "            outputs.video_out[i] = stream\n",
    "        \n",
    "\n",
    "        return True\n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    HandWashDetection().run()\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Upload Lambda and Create Lambda Function\n",
    "\n",
    "* A lambda is already provided and ready for use in the lambda folder (zip file)\n",
    "* Use this code snippet to upload and publish it to Lambda Service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet uses boto3 to create an IAM role named LambdaBasicExecution with basic \n",
    "lambda execution permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam_client = boto3.client(\"iam\")\n",
    "\n",
    "iam_client.create_role(\n",
    "    RoleName=\"HandWashingExecutionRole\",\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python snippet will use the resources above to create a new AWS Lambda function called HandWashingLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -o  ../Lambda/HandWashingLambda.zip  ../Lambda/classes.py ../Lambda/HandWashDetection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client(\"lambda\")\n",
    "\n",
    "with open(\n",
    "    \"../Lambda/HandWashingLambda.zip\", \"rb\"\n",
    ") as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "role = iam_client.get_role(RoleName=\"HandWashingExecutionRole\")\n",
    "response_create_function = lambda_client.create_function(\n",
    "    FunctionName=\"HandWashingDetectionLambda\",\n",
    "    Runtime=\"python3.7\",\n",
    "    Role=role[\"Role\"][\"Arn\"],\n",
    "    Handler=\"HandWashDetection.main\",\n",
    "    Code=dict(ZipFile=zipped_code),\n",
    "    Timeout=120,\n",
    "    MemorySize=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an ARN?** : Amazon Resource Names (ARNs) uniquely identify AWS resources.\n",
    "\n",
    "The following Python snippet will publish the Lambda Function we created above, and return an ARN with a version. \n",
    "\n",
    "This version arn can be used to go directly to the Panorama console and deploy this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.publish_version(FunctionName=\"HandWashingDetectionLambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the details of the lambda function that was just published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_arn = response[\"FunctionArn\"]\n",
    "function_arn_version = list(response[\"FunctionArn\"].split(\":\"))[-1]\n",
    "lambda_url = (\n",
    "    \"https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/\"\n",
    "    + response[\"FunctionName\"]\n",
    "    + \"/versions/\"\n",
    "    + response[\"Version\"]\n",
    "    + \"?tab=configuration\"\n",
    ")\n",
    "\n",
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Upload Model to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_model_to_s3(model, bucket = 'aws-panorama-models-bucket'):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    \n",
    "    key = '../../Models/' + model\n",
    "    \n",
    "    s3.Object(bucket, model).put(Body=open(key, 'rb'))\n",
    "    \n",
    "    bucket_name = bucket\n",
    "    \n",
    "    \n",
    "    location = boto3.client('s3').get_bucket_location(Bucket='aws-panorama-models-bucket')['LocationConstraint']\n",
    "    url = \"s3://{}/{}\".format(bucket_name, key)\n",
    "    \n",
    "    printmd(\"**S3 Path** : **{}**\".format(url), color=\"black\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'resnet101_v1b_kinetics400.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 : Deploy the Application\n",
    "\n",
    "The Lambda is now created and published. You are now ready to deploy your model and the published lambda function, to the Manhattan Panorama device\n",
    "\n",
    "The instructions to deploy are linked below\n",
    "\n",
    "[Creating Application Instructions Here](https://docs.aws.amazon.com/panorama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helpful information about Lambda and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")\n",
    "\n",
    "printmd(\"**Inputs for resnet101_v1b_kinetics400 Model**\", color=\"black\")\n",
    "print('     ')\n",
    "printmd(\"**Input Name** : **{}**\".format('data'), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format([1,3,224, 224]), color=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
