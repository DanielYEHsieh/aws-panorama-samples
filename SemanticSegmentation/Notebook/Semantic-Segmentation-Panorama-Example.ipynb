{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Create A Lambda Function for Semantic Segmenation in AWS Panorama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this notebook accomplishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to create and deploy a lambda function that is used for a semantic segmentation application using the AWS Panorama service. \n",
    "- You will learn the parts of an Panorama application and what they are used for. \n",
    "- Then you will put it all together to simulate a semantic segmentation application within the environment of this notebook. \n",
    "\n",
    "The application uses a pretrained semantic segmentation model from GluonCV/MXNET. You can find more information and tutorials for semantic segmentation using [MXNET & GluonCV](https://gluon-cv.mxnet.io/build/examples_segmentation/index.html) tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires the following packages\n",
    "- mxnet >= 1.6.0\n",
    "- gluoncv >= 0.6.0\n",
    "- opencv-python == 4.4.0\n",
    "- matplotlib\n",
    "- boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre -Requisites**:\n",
    "* Sagemaker Instance created with the right role (Policies needed IOT, Lambda and S3, IAM Full Access) ( Add Doc here)\n",
    "\n",
    "\n",
    "**Frames to Process**:\n",
    "\n",
    "* By default, we only process 10 frames from the video. If you want to increase this, please change this value in /panorama_sdk/panoramasdk.py and change frames_to_process = 10 to a value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Semantic Segmenation\n",
    "\n",
    "- Instance segmentation is task of detecting objects in an image and segmenting the . \n",
    "- This type of deep learning task is used in autonomous driving, medical imaging analysis and terrain classification in sattelite images.\n",
    "- In other examples, customer's may use this to detect the presence of smoke or clouds in their environments.\n",
    "\n",
    "## 1.1 The end to end process for Semantic Segmentation inference is relatively simple. \n",
    "- It consists of taking an image (or in this case a video frame) and preprocessing it\n",
    "- performing model inference\n",
    "- then using the output masks to either visualize or perform further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Run the next cell to define the preprocessing functions. \n",
    "As you can see, these functions just resize the image and normalize the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_use = \"man_and_tree.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "\n",
    "import panoramasdk\n",
    "import jupyter_utils\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mask(img, predict, alpha=0.5):\n",
    "    classes = np.unique(predict)\n",
    "    w, h, _ = img.shape\n",
    "    for i, cla in enumerate(classes):\n",
    "        color = np.random.random(3) * 255\n",
    "        mask = np.repeat((predict == cla)[:, :, np.newaxis], repeats=3, axis=2).astype(\n",
    "            \"uint8\"\n",
    "        )\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = mask * 255\n",
    "        mask = cv2.resize(mask, (h, w))\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        img = cv2.drawContours(img, contours, -1, color, -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "\n",
    "    img = img.astype(np.float32) / 255.0  # converting array of ints to floats\n",
    "    img_a = img[:, :, 0]\n",
    "    img_b = img[:, :, 1]\n",
    "    img_c = img[:, :, 2]\n",
    "\n",
    "    # Extracting single channels from 3 channel image\n",
    "    # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "\n",
    "    # normalizing per channel data:\n",
    "    img_a = (img_a - mean[0]) / std[0]\n",
    "    img_b = (img_b - mean[1]) / std[1]\n",
    "    img_c = (img_c - mean[2]) / std[2]\n",
    "\n",
    "    # putting the 3 channels back together:\n",
    "    x1 = [[[], [], []]]\n",
    "    x1[0][0] = img_a\n",
    "    x1[0][1] = img_b\n",
    "    x1[0][2] = img_c\n",
    "\n",
    "    # x1 = mx.nd.array(np.asarray(x1))\n",
    "    x1 = np.asarray(x1)\n",
    "\n",
    "    return x1\n",
    "\n",
    "def preprocess(img, shape=(512, 512)):\n",
    "\n",
    "    resized = cv2.resize(img, shape)  # (h, w)\n",
    "    x1 = normalize(resized)\n",
    "    return x1, resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Next, read in the image and transform it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"persons.jpg\"\n",
    "img = cv2.imread(filename)            \n",
    "\n",
    "            \n",
    "#x1 = self.preprocess(person_image)\n",
    "print(\"Processing Image...\")\n",
    "x1, orig_img = preprocess(img, (400, 500))\n",
    "print(\"Processed Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Load the model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = panoramasdk.model()\n",
    "model.open('fcn_resnet101_voc', 1)\n",
    "\n",
    "\n",
    "# Create input and output arrays.\n",
    "mask_info = model.get_output(0)\n",
    "prob_info = model.get_output(1)\n",
    "\n",
    "mask_array = np.empty(mask_info.get_dims(), dtype=mask_info.get_type())\n",
    "prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Run the next cell to define the mask plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference on the new frame.\n",
    "print(\"Performing Detector Inference\")\n",
    "model.batch(0, x1)\n",
    "model.flush()\n",
    "print(\"Inference Completed.\")\n",
    "\n",
    "# Get the results.\n",
    "resultBatchSet = model.get_result()\n",
    "\n",
    "mask_batch = resultBatchSet.get(0)\n",
    "prob_batch = resultBatchSet.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Plot the mask boundaries in the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_batch.get(0, mask_array)\n",
    "prob_batch.get(1, prob_array)\n",
    "\n",
    "masks = mask_array\n",
    "print(\"Output shape:\", masks.shape)\n",
    "masks = np.squeeze(np.argmax(masks, 1))\n",
    "print(\"Processed output shape:\", masks.shape)\n",
    "\n",
    "\n",
    "image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image = plot_mask(image, masks)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Panorama application, it might be more useful to show boundaries around the pixels instead of the silloutes. This can be done by modifying the following line from the `plot_mask` function.\n",
    "\n",
    "```\n",
    "...\n",
    "img = cv2.drawContours(img, contours, -1, color, 10)\n",
    "...\n",
    "```\n",
    "\n",
    "This is how you will implement the plotting function in the Panorama application later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to building the Panorama application code that goes inside the lambda function is understanding what parts of **Panorama Application Class** are necessary. Panorama service uses objects of this class to manage the model, preprocess images and perform inference.\n",
    "\n",
    "- At a high level, the raw code cell below is what the application class looks like. \n",
    "- The `interface`, `init`, and `entry` methods are the basic necessary components to building the application class.\n",
    "\n",
    "**Note**: In addition to these methods, you can add custom methods either to the class or the script globally. You will see both illustrated in the final lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class people_mask(panoramasdk.base):\n",
    "    def interface(self):\n",
    "        # defines the parameters that interface with other services from Panorama\n",
    "        \n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        # defines the attributes such as arrays and model objects that will be used in the application\n",
    "        \n",
    "    def entry(self, inputs, outputs):\n",
    "        # defines the application logic responsible for predicting using the inputs and handles what to do \n",
    "        # with the outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining the `interface` method\n",
    "The first method to define is the `interface` method. This is an important part of the Panorama application because this is how the application interacts with the resources from the Panorama service. When creating an Panorama Application in the Panorama console, you should have specified the model that will be used for predicting, the IP camera input streams and where the application's output is going. \n",
    "- The Panorama Service compiles your pretrained model and prepares it to be deployed to Manhattan device. \n",
    "- Camera inputs are defined so that each camera's stream images are passed to the application to be predicted on by the AWS Lambda function's application code.\n",
    "- Outputs are defined to receive the model's post-processed output. This is usually set to output to HDMI for visualizing results.\n",
    "\n",
    "Lastly, the interface method then defines these inputs and outputs within the Panorama application class so that the application has access to them in the `entry` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def interface(self):\n",
    "    return {\n",
    "        \"parameters\":\n",
    "            (\n",
    "                (\"model\", \"segmodel\", \"Model for segmenting pixels\", \"segmentation-model2\"),\n",
    "                (\"int\", \"batch_size\", \"Model batch size\", 1),\n",
    "            ),\n",
    "        \"inputs\":\n",
    "            (\n",
    "                (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "            ),\n",
    "        \"outputs\":\n",
    "            (\n",
    "                (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "            )\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `interface` method returns a dictionary of items that define parameters, inputs and outputs. \n",
    "- These items consist of an array of tuples. \n",
    "- Each tuple follows this schema:\n",
    "\n",
    "(`data type`, `variable name`, `description`, `value`)\n",
    "\n",
    "**Tip**: You can add your own parameters to the `parameters` object, that can be later used in the `init` method. \n",
    "\n",
    "For this example, you will set the value of the `segmodel` parameter to **\"segmentation-model\"**, and in the future you will set it to the name of the model you have defined to be part of the Panorama application in the Panorama Console. Batch size will always be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the `init` method\n",
    "\n",
    "The init method serves a similar purpose to traditional `__init__` methods used in python classes. \n",
    "- The difference here is that the initialization parameters come from the `parameters` object passed. \n",
    "- Using this `parameters` object, you can initialize the model that has been previously uploaded in the Panorama Console. \n",
    "- You will also define array containers for the model's output based on the model's output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def init(self, parameters, inputs, outputs):\n",
    "    try:\n",
    "        self.frame_num = 0\n",
    "\n",
    "        # Load model from the specified directory.\n",
    "        print(\"loading the model...\")\n",
    "        self.model = panoramasdk.model()\n",
    "        self.model.open(parameters.segmodel, 1)\n",
    "        print(\"model loaded\")\n",
    "\n",
    "        # Create input and output arrays.\n",
    "        mask_info = self.model.get_output(0)\n",
    "        prob_info = self.model.get_output(1)\n",
    "\n",
    "        self.mask_array = np.empty(mask_info.get_dims(), dtype=mask_info.get_type())\n",
    "        self.prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception: {}\".format(e))\n",
    "        return False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note, with the code below\n",
    "```\n",
    "...\n",
    "self.model = panoramasdk.model()\n",
    "self.model.open(parameters.segmodel, 1)\n",
    "...\n",
    "```\n",
    "First, an `panoramasdk.model` object is initialized, then the pretrained model is loaded into that object. \n",
    "- It knows what model to load based on the parameter you set in `interface` for the corrresponding model parameter. \n",
    "- In this notebook you will use a different pattern so that it can demo-ed in a notebook environment, but it will behave similarly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Defining the `entry` method\n",
    "\n",
    "The entry method is the *entry* point for the Panorama application; and it contains all the logic to perform for each video input, for each frame. This is where you will perform the preprocessing, inferencing, and post-processing. \n",
    "For the segmentation task, it's as simple as the following steps:\n",
    "\n",
    "1. preprocess the image input\n",
    "2. perform inference\n",
    "3. get the results\n",
    "4. plot the masks received from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def entry(self, inputs, outputs):\n",
    "\n",
    "    for i in range(len(inputs.video_in)):\n",
    "\n",
    "        stream = inputs.video_in[i]\n",
    "\n",
    "        person_image = stream.image\n",
    "\n",
    "        #x1 = self.preprocess(person_image)\n",
    "        print(\"Processing Image...\")\n",
    "        x1, orig_img = preprocess(person_image, (400, 500))\n",
    "        print(\"Processed Image\")\n",
    "\n",
    "\n",
    "        # Do inference on the new frame.\n",
    "        print(\"Performing Detector Inference\")\n",
    "        self.model.batch(0, x1)\n",
    "        self.model.flush()\n",
    "        print(\"Inference Completed.\")\n",
    "\n",
    "        # Get the results.\n",
    "        resultBatchSet = self.model.get_result()\n",
    "\n",
    "        mask_batch = resultBatchSet.get(0)\n",
    "        prob_batch = resultBatchSet.get(1)\n",
    "\n",
    "        mask_batch.get(0, self.mask_array)\n",
    "        prob_batch.get(0, self.prob_array)\n",
    "\n",
    "        masks = np.squeeze(np.argmax(self.mask_array, 1))\n",
    "        _ = plot_mask(stream.image, masks)\n",
    "\n",
    "\n",
    "        self.model.release_result(resultBatchSet)\n",
    "        outputs.video_out[i] = stream\n",
    "\n",
    "    return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting it all together\n",
    "\n",
    "A version of the lambda code compatible within a notebook environment has been written for you below. Run the next cell to visualize the application's output. \n",
    "\n",
    "**Note**: Custom functions are defined globablly, outside of the class. You could also defined the same custom functions as part of the `segmentation` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_utils.change_video_source(video_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "def plot_mask(img, predict, alpha=0.5):\n",
    "    classes = np.unique(predict)\n",
    "    w, h, _ = img.shape\n",
    "    for i, cla in enumerate(classes):\n",
    "        color = np.random.random(3) * 255\n",
    "        mask = np.repeat((predict == cla)[:, :, np.newaxis], repeats=3, axis=2).astype(\n",
    "            \"uint8\"\n",
    "        )\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = mask * 255\n",
    "        mask = cv2.resize(mask, (h, w))\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        img = cv2.drawContours(img, contours, -1, color, -1)\n",
    "    return img\n",
    "    \n",
    "def preprocess(img, shape=(512, 512)):\n",
    "    \n",
    "    resized = cv2.resize(img, shape) # (h, w)\n",
    "    x1 = normalize(resized)\n",
    "    return x1, resized\n",
    "        \n",
    "def normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "\n",
    "    img = img.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "    img_a = img[:, :, 0]\n",
    "    img_b = img[:, :, 1]\n",
    "    img_c = img[:, :, 2]\n",
    "\n",
    "    # Extracting single channels from 3 channel image\n",
    "    # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "\n",
    "    # normalizing per channel data:\n",
    "    img_a = (img_a - mean[0]) / std[0]\n",
    "    img_b = (img_b - mean[1]) / std[1]\n",
    "    img_c = (img_c - mean[2]) / std[2]\n",
    "\n",
    "    # putting the 3 channels back together:\n",
    "    x1 = [[[], [], []]]\n",
    "    x1[0][0] = img_a\n",
    "    x1[0][1] = img_b\n",
    "    x1[0][2] = img_c\n",
    "\n",
    "    # x1 = mx.nd.array(np.asarray(x1))\n",
    "    x1 = np.asarray(x1)\n",
    "    \n",
    "    return x1    \n",
    "\n",
    "class segmentation(panoramasdk.base):\n",
    "    \n",
    "    def interface(self):\n",
    "        return {\n",
    "            \"parameters\":\n",
    "                (\n",
    "                    (\"model\", \"segmodel\", \"Model for people detecting\", \"fcn_resnet101_voc\"),\n",
    "                    (\"int\", \"batch_size\", \"Model batch size\", 1),\n",
    "                ),\n",
    "            \"inputs\":\n",
    "                (\n",
    "                    (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "                ),\n",
    "            \"outputs\":\n",
    "                (\n",
    "                    (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "                )\n",
    "        }\n",
    "\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        try:\n",
    "            self.frame_num = 0\n",
    "            \n",
    "            # Load model from the specified directory.\n",
    "            print(\"loading the model...\")\n",
    "            self.model = panoramasdk.model()\n",
    "            self.model.open(parameters.segmodel, 1)\n",
    "            print(\"model loaded\")\n",
    "            \n",
    "            # Create input and output arrays.\n",
    "            mask_info = self.model.get_output(0)\n",
    "            prob_info = self.model.get_output(1)\n",
    "        \n",
    "            self.mask_array = np.empty(mask_info.get_dims(), dtype=mask_info.get_type())\n",
    "            self.prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "            \n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            return False\n",
    "        \n",
    "    def entry(self, inputs, outputs):\n",
    "\n",
    "        for i in range(len(inputs.video_in)):\n",
    "\n",
    "            stream = inputs.video_in[i]\n",
    "            person_image = stream.image\n",
    "            \n",
    "            \n",
    "            #print(\"Processing Image...\")\n",
    "            x1, orig_img = preprocess(person_image, (400, 500))\n",
    "            #print(\"Processed Image\")\n",
    "            \n",
    "            \n",
    "            # Do inference on the new frame.\n",
    "            #print(\"Performing Detector Inference\")\n",
    "            self.model.batch(0, x1)\n",
    "            self.model.flush()\n",
    "            #print(\"Inference Completed.\")\n",
    "\n",
    "            # Get the results.\n",
    "            resultBatchSet = self.model.get_result()\n",
    "\n",
    "            mask_batch = resultBatchSet.get(0)\n",
    "            prob_batch = resultBatchSet.get(1)\n",
    "            \n",
    "            mask_batch.get(0, self.mask_array)\n",
    "            prob_batch.get(0, self.prob_array)\n",
    "        \n",
    "            masks = np.squeeze(np.argmax(self.mask_array, 1))\n",
    "            \n",
    "            person_image = plot_mask(person_image, masks)\n",
    "        \n",
    "        \n",
    "            self.model.release_result(resultBatchSet)\n",
    "            outputs.video_out[i] = stream\n",
    "            \n",
    "            \n",
    "\n",
    "        return True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    segmentation().run()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying Lambda\n",
    "\n",
    "As mentioned previously, code from above is adapted to run within a notebook environment. You can find the actual lambda code in `segmentation.py` found in the **Lambda** directory. In order to deploy a lambda function from this notebook, you will need the `semantic-segmentation.zip` file that contains the following files:\n",
    "- segmentation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define boto sessions\n",
    "Run the following cell to initialize boto3 to interface with AWS Lambda service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create the role for Lambda Execution\n",
    "Running the next cell will create an execution role that will be used to deploy your lambda zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam_client = boto3.client(\"iam\")\n",
    "\n",
    "iam_client.create_role(\n",
    "    RoleName=\"SemanticSegExecutionRole\",\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create the AWS Lambda function\n",
    "Next, the following cell creates the lambda function in your AWS Lambda service and uploads your zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -o  ../Lambda/segmentation.zip  ../Lambda/segmentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda client\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "with open('../Lambda/segmentation.zip', 'rb') as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "role = iam_client.get_role(RoleName='SemanticSegExecutionRole')\n",
    "response_create_function = lambda_client.create_function(\n",
    "  FunctionName='SegmentationSegmentationLambda',\n",
    "  Runtime='python3.7',\n",
    "  Role=role['Role']['Arn'],\n",
    "  Handler='segmentation.main',\n",
    "    Code=dict(ZipFile=zipped_code),\n",
    "    Timeout=120,\n",
    "    MemorySize=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an ARN?** : Amazon Resource Names (ARNs) uniquely identify AWS resources.\n",
    "\n",
    "The following Python snippet will publish the Lambda Function we created above, and return an ARN with a version. \n",
    "\n",
    "This version arn can be used to go directly to the Panorama console and deploy this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Publish Lambda\n",
    "Lastly, publish the latest version of you lambda function so that it's available to use in the Panorama Application console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    \"\"\"\n",
    "    Helper Function for Fomatting Output\n",
    "    \"\"\"\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish Lambda\n",
    "response = lambda_client.publish_version(\n",
    "      FunctionName='SegmentationSegmentationLambda'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the details of the lambda function that was just published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_arn = response[\"FunctionArn\"]\n",
    "function_arn_version = list(response[\"FunctionArn\"].split(\":\"))[-1]\n",
    "lambda_url = (\n",
    "    \"https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/\"\n",
    "    + response[\"FunctionName\"]\n",
    "    + \"/versions/\"\n",
    "    + response[\"Version\"]\n",
    "    + \"?tab=configuration\"\n",
    ")\n",
    "\n",
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 : Upload Model to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_model_to_s3(model, bucket = 'aws-panorama-models-bucket'):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    \n",
    "    key = '../../Models/' + model\n",
    "    \n",
    "    s3.Object(bucket, model).put(Body=open(key, 'rb'))\n",
    "    \n",
    "    bucket_name = bucket\n",
    "    \n",
    "    \n",
    "    location = boto3.client('s3').get_bucket_location(Bucket='aws-panorama-models-bucket')['LocationConstraint']\n",
    "    url = \"s3://{}/{}\".format(bucket_name, model)\n",
    "    \n",
    "    printmd(\"**S3 Path** : **{}**\".format(url), color=\"black\")\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'fcn_resnet101_voc.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 : Deploy the Application\n",
    "\n",
    "The Lambda is now created and published. You are now ready to deploy your model and the published lambda function, to the Panorama device\n",
    "\n",
    "The instructions to deploy are linked below\n",
    "\n",
    "[Creating Application Instructions Here](https://docs.aws.amazon.com/panorama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
