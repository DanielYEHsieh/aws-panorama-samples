{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to detect Social Distancing violations using AWS Panorama\n",
    "\n",
    "**Goal of this Notebook** :\n",
    "\n",
    "* Aid an Panorama developer prototype their application before creating the AWS Lambda for Panorama\n",
    "* Creating an MXNET application in the same structure as the AWS Lambda for Panorama\n",
    "* Create and Deploy the AWS Lambda for Panorama from this notebook\n",
    "\n",
    "**What this Notebook accomplishes?** :\n",
    "* Detect Distance between people in a frame\n",
    "* Red Bounding Boxes are drawn for people not adhering to Social Distancing\n",
    "* Green Bounding Boxes are drawn for people who are adhering to Social Distancing\n",
    "\n",
    "\n",
    "**Useful Resources to aid your development**:\n",
    "* [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre -Requisites**:\n",
    "* Sagemaker Instance created with the right role (Policies needed IOT, Lambda and S3, IAM Full Access) ( Add Doc here)\n",
    "\n",
    "\n",
    "**Frames to Process**:\n",
    "\n",
    "\n",
    "* By default, we only process 10 frames from the video. If you want to increase this, please change this value in /panorama_sdk/panoramasdk.py and change frames_to_process = 10 to a value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Imports\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_use = \"TownCentreXVID.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "\n",
    "import jupyter_utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from gluoncv import model_zoo, data, utils\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "from IPython.display import display, Math, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dotdict` class is defined so we can get access to dictionary attributes with a `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Explaining the approach\n",
    "----------------\n",
    "\n",
    "- Use a GluonCV VOC model to get all person bounding boxes\n",
    "- Loop thru the bounding boxes and determine the distance between all person pairs (More on this later)\n",
    "- Return distance between all people\n",
    "- If distance falls below Threshold value, we return a red bounding box, else green\n",
    "\n",
    "\n",
    "**Distance Calculation**\n",
    "\n",
    "- To calculate distance , a size mask is created for the view provided by the camera\n",
    "- For every area in the image that has a person in it, calculate an image distance that correspond to 6 ft\n",
    "- Use the above calculation as a conversion factor to convert distances to ft\n",
    "\n",
    "Distance Calculation Formula\n",
    "\n",
    "- Taking the Top and Left bounding box coordinates of each person identified, we calculate the following between two people A and B\n",
    "\n",
    "$$distance = \\sqrt(({TopB - TopA}^2) - ({LeftB - LeftA}^2))$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This below picture gives a great example\n",
    "\n",
    "![Distance Calc4](DistanceCalc4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Main Function\n",
    "----------------\n",
    "\n",
    "**Note**: \n",
    "\n",
    "* The code structure below is as close to the lambda structure as possible, but is **NOT** the actual Lambda.\n",
    "\n",
    "* To see and compare the actual lambda to this code, please open the HandWashDetection.py file in the Lambda folder.\n",
    "\n",
    "The Panorama Lambda function has the following structure\n",
    "\n",
    "**Tip** : Actual Lambda inherits base class called panoramasdk.base instead of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lambda skeleton\n",
    "\n",
    "\n",
    "class AwsPanoramaSD(object):\n",
    "    def interface(self):\n",
    "        # defines the parameters that interface with other services from Panorama\n",
    "        return\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        # defines the attributes such as arrays and model objects that will be used in the application\n",
    "        return\n",
    "\n",
    "    def entry(self, inputs, outputs):\n",
    "        # defines the application logic responsible for predicting using the inputs and handles what to do\n",
    "        # with the outputs\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Notes**:\n",
    "\n",
    "- This runs only for 20 frames for now, if you want to change that, please change the self.num_frames in the init function. \n",
    "\n",
    "- This is not the case with the actual lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_utils.change_video_source(video_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import boto3\n",
    "import math \n",
    "import datetime\n",
    "\n",
    "\n",
    "# SD based imports\n",
    "import ModelOutput as jm\n",
    "import socialDistance as sd\n",
    "import socialDistanceUtils as sdu\n",
    "\n",
    "red    = (0,0,255)\n",
    "green  = (0,255,0)\n",
    "black  = (0,0,0)\n",
    "white  = (255,255,255)\n",
    "text_color = black\n",
    "\n",
    "num_frames = 1000000000\n",
    "mask_frequency = 10\n",
    "\n",
    "\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "\n",
    "class AwsPanoramaSD(panoramasdk.base):\n",
    "    \n",
    "    def interface(self):\n",
    "        return {\n",
    "                \"parameters\":\n",
    "                (\n",
    "                    (\"model\", \"person_detection\", \"Model for detecting persons\", \"ssd_512_resnet50_v1_voc\"), \n",
    "                    (\"float\", \"threshold\", \"Detection threshold\", 0.10),\n",
    "                    (\"int\", \"batch_size\", \"Model batch size\", 1),\n",
    "                    (\"float\", \"person_index\", \"person index based on dataset used\", 14)\n",
    "                ),\n",
    "                \"inputs\":\n",
    "                (\n",
    "                    (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "                ),\n",
    "                \"outputs\":\n",
    "                (\n",
    "                    (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "                    \n",
    "                ) \n",
    "            }\n",
    "        \n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            print('Loading Model')\n",
    "            self.model = panoramasdk.model()\n",
    "            self.model.open(parameters.person_detection, 1)\n",
    "            print('Model Loaded')\n",
    "\n",
    "            # Detection probability threshold.\n",
    "            self.threshold = parameters.threshold\n",
    "\n",
    "            # Frame Number Initialization\n",
    "            self.frame_num = 0\n",
    "\n",
    "            # Number of People\n",
    "            self.number_people = 0\n",
    "\n",
    "            # Bounding Box Colors\n",
    "            self.colours = np.random.rand(32, 3)\n",
    "\n",
    "            # Person Index for Model from parameters\n",
    "            self.person_index = parameters.person_index\n",
    "\n",
    "            \n",
    "            self.boxes = []\n",
    "            self.frame_num = 0\n",
    "            self.send_images = True\n",
    "            \n",
    "            # SD Code 2\n",
    "            self._batch_frame_count = 0\n",
    "            self._frame_count = 0\n",
    "            self._cam_standing_people = []\n",
    "            self._curr_refs = []\n",
    "            self._total_size_mask_count = 0\n",
    "            self._cam_left = -1\n",
    "            self._size_mask = []\n",
    "            self.mask_frequency = 10\n",
    "            self.num_frames = 20\n",
    "\n",
    "            # Create input and output arrays.\n",
    "            class_info = self.model.get_output(0)\n",
    "            prob_info = self.model.get_output(1)\n",
    "            rect_info = self.model.get_output(2)\n",
    "\n",
    "            self.class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "            self.prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "            self.rect_array = np.empty(rect_info.get_dims(), dtype=rect_info.get_type())\n",
    "\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            return False\n",
    "\n",
    "    def preprocess(self, img, size):\n",
    "        \n",
    "        resized = cv2.resize(img, (size, size))\n",
    "        mean = [0.485, 0.456, 0.406]  # RGB\n",
    "        std = [0.229, 0.224, 0.225]  # RGB\n",
    "        \n",
    "        img = resized.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "        img_a = img[:, :, 0]\n",
    "        img_b = img[:, :, 1]\n",
    "        img_c = img[:, :, 2]\n",
    "        \n",
    "        # Extracting single channels from 3 channel image\n",
    "        # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "        # normalizing per channel data:\n",
    "        img_a = (img_a - mean[0]) / std[0]\n",
    "        img_b = (img_b - mean[1]) / std[1]\n",
    "        img_c = (img_c - mean[2]) / std[2]\n",
    "        \n",
    "        # putting the 3 channels back together:\n",
    "        x1 = [[[], [], []]]\n",
    "        x1[0][0] = img_a\n",
    "        x1[0][1] = img_b\n",
    "        x1[0][2] = img_c\n",
    "        x1 = np.asarray(x1)\n",
    "        \n",
    "        return x1\n",
    "    \n",
    "    def get_number_persons(self, class_data, prob_data):\n",
    "        \n",
    "        # get indices of people detections in class data\n",
    "        person_indices = [i for i in range(len(class_data)) if int(class_data[i]) == self.person_index]\n",
    "        # use these indices to filter out anything that is less than 95% threshold from prob_data\n",
    "        prob_person_indices = [i for i in person_indices if prob_data[i] >= self.threshold]\n",
    "        return prob_person_indices\n",
    "    \n",
    "    \n",
    "    def blur_bounding_box(self, person_image, bbox,sigma = 3.5): \n",
    "        img = person_image\n",
    "        img_arr = img.copy()\n",
    "        img_w, img_h = img.shape[1],img.shape[0]\n",
    "        nominal_box_area = img_h * img_w * 0.1\n",
    "        blur_sigma = sigma\n",
    "        for boxes in bbox:\n",
    "            try:\n",
    "                xmin, ymin, xmax, ymax = [int(x) for x in boxes]\n",
    "                print('BBox input into function {}'.format([xmin, ymin, xmax, ymax]))\n",
    "                ymin = int(((ymin/512.0)*img_h))\n",
    "                xmin = int(((xmin/512.0)*img_w))\n",
    "                ymax = int(((ymax/512.0)*img_h))\n",
    "                xmax = int(((xmax/512.0)*img_w))\n",
    "                print('BBox input into function after scaling {}'.format([xmin, ymin, xmax, ymax]))    \n",
    "                # blurring \n",
    "                box_area = (xmax - xmin) * (ymax - ymin)\n",
    "                sigma_scaled = blur_sigma * math.sqrt(box_area/nominal_box_area)\n",
    "                sigma_clamped = min(5.5, max(1.0, sigma_scaled))\n",
    "                bbox_img = img_arr[ymin:ymax, xmin:xmax, :]\n",
    "                img_arr[ymin:ymax, xmin:xmax, :] = cv2.GaussianBlur(bbox_img, (0, 0), sigma_clamped)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return img_arr\n",
    "        \n",
    "\n",
    "    def different_enough(self, a, b):\n",
    "        try:\n",
    "            a_bb = a['BoundingBox']\n",
    "            b_bb = b['BoundingBox']\n",
    "        \n",
    "            h_diff = abs(a_bb['Height'] - b_bb['Height'])\n",
    "            w_diff = abs(a_bb['Width']  - b_bb['Width'])\n",
    "            t_diff = abs(a_bb['Top']    - b_bb['Top'])\n",
    "            l_diff = abs(a_bb['Left']   - b_bb['Left'])\n",
    "        \n",
    "            total_other_diff = h_diff + w_diff + l_diff\n",
    "            if (t_diff > 0.10) and (total_other_diff > 0.30):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print('Different enough exception is {}'.format(e))\n",
    "            return False\n",
    "    \n",
    "    def add_distinct_people(self, all_people, new_people):\n",
    "        if len(all_people) == 0:\n",
    "            return new_people \n",
    "        else:\n",
    "            tmp_people = all_people.copy()\n",
    "            for np in new_people:\n",
    "                is_diff = False\n",
    "                for ap in all_people:\n",
    "                    is_this_one_diff = self.different_enough(np, ap)\n",
    "                    if is_this_one_diff:\n",
    "                        is_diff = True\n",
    "                        break\n",
    "                if is_diff:\n",
    "                    tmp_people.append(np)\n",
    "            return tmp_people\n",
    "\n",
    "    \n",
    "    def entry(self, inputs, outputs):\n",
    "\n",
    "        for i in range(len(inputs.video_in)):\n",
    "            stream = inputs.video_in[i]\n",
    "            person_image = stream.image\n",
    "            w, h, c = person_image.shape\n",
    "            \n",
    "            sending_image = person_image.copy()\n",
    "            redacted_image = person_image.copy()\n",
    "                \n",
    "             # SD Code 1\n",
    "            _frame = person_image.copy()\n",
    "            _image_shape, _size_mask_shape = sdu.get_shapes(_frame)\n",
    "                        \n",
    "\n",
    "            # Pre Process Frame\n",
    "            x1 = self.preprocess(person_image, 512)\n",
    "                                    \n",
    "            # Do inference on the new frame.\n",
    "            self.model.batch(0, x1)\n",
    "            self.model.flush()\n",
    "            \n",
    "            # Get the results.\n",
    "            resultBatchSet = self.model.get_result()\n",
    "            \n",
    "            class_batch = resultBatchSet.get(0)\n",
    "            prob_batch = resultBatchSet.get(1)\n",
    "            rect_batch = resultBatchSet.get(2)\n",
    "\n",
    "            class_batch.get(0, self.class_array)\n",
    "            prob_batch.get(1, self.prob_array)\n",
    "            rect_batch.get(2, self.rect_array)\n",
    "\n",
    "            class_data = self.class_array[0]\n",
    "            prob_data = self.prob_array[0]\n",
    "            rect_data = self.rect_array[0]\n",
    "            \n",
    "            \n",
    "            class_data2 = self.class_array\n",
    "            prob_data2 = self.prob_array\n",
    "            rect_data2 = self.rect_array               \n",
    "\n",
    "            person_indices = self.get_number_persons(class_data,prob_data)\n",
    "\n",
    "            # find people in this frame\n",
    "            _img_all_people, _img_standing_people = sdu.get_standing_people(_frame, _image_shape, _size_mask_shape, x1, class_data2, prob_data2, rect_data2)\n",
    "            _num_total_people = len(_img_all_people)\n",
    "            _min_distance = sd.MAX_DISTANCE\n",
    "            _safe_cat = 'LessThanTwoPeople'\n",
    "            \n",
    "            \n",
    "            # every N seconds, if we still need a better size mask, try to improve it\n",
    "            if self._batch_frame_count < mask_frequency:\n",
    "                self._batch_frame_count += 1\n",
    "                \n",
    "            elif (len(self._cam_standing_people) < sdu.MAX_STANDING_REFS_NEEDED) or (self._total_size_mask_count == 0):\n",
    "                self._batch_frame_count = 0\n",
    "                # 1. add to cumulative list of standing people for this camera\n",
    "                self._cam_standing_people = self.add_distinct_people(self._cam_standing_people, _img_standing_people)\n",
    "                # 2. once we have at least MIN_STANDING_REFS_PER_CAM people in our running list\n",
    "                if (len(self._cam_standing_people) > sdu.MIN_STANDING_REFS_PER_CAM):\n",
    "                    # 2a. generate the best size mask for the standing people thus far\n",
    "                    _curr_camera_config, _curr_best_rmse, _curr_size_mask_count = \\\n",
    "                        sdu.gen_best_size_mask(self._cam_standing_people, _image_shape, _size_mask_shape)\n",
    "\n",
    "                    if _curr_size_mask_count > 0:\n",
    "                        self._curr_refs = _curr_camera_config['MaskReferenceSizes']\n",
    "                    self._total_size_mask_count += _curr_size_mask_count\n",
    "\n",
    "                    #print('total masks: {}, latest rmse: {}, refs: {}'.format(self._total_size_mask_count[i],_curr_best_rmse,self._curr_refs[i]))\n",
    "                    if _curr_size_mask_count > 0:\n",
    "                        self._cam_left   = _curr_camera_config['CameraLeft']\n",
    "                        _cam_height = _curr_camera_config['CameraHeight']\n",
    "                        self._size_mask = np.asarray(_curr_camera_config['SizeMask'])\n",
    "\n",
    "                                    \n",
    "            if (self._total_size_mask_count > 0):\n",
    "                _verbose = False\n",
    "                _likely_people, _proximity_list = sd.detect_distances(_img_all_people, self._size_mask, _image_shape,\n",
    "                                                                      _cam_height, _verbose)\n",
    "                _min_distance     = sdu.min_distance_from_list(_proximity_list)\n",
    "                _num_unsafe_pairs = sdu.get_num_unsafe_pairs(_proximity_list)\n",
    "                _num_total_people = len(_likely_people)\n",
    "\n",
    "                if _num_total_people < 2:\n",
    "                    _safe_cat = 'LessThanTwoPeople'\n",
    "                elif _min_distance > sdu.MIN_SAFE_DISTANCE:\n",
    "                    _safe_cat = 'AppropriateDistance'\n",
    "                else:\n",
    "                    _safe_cat = 'ReducedDistance'\n",
    "\n",
    "            \n",
    "            else:\n",
    "                _proximity_list = [] \n",
    "                self._curr_refs      = []\n",
    "                _likely_people  = []\n",
    "                self._curr_refs.append({'HeightImageRatio': 0, 'AspectRatio': 0, 'FromImage': '', 'GridPos': [0,0]})\n",
    "                self._curr_refs.append({'HeightImageRatio': 0, 'AspectRatio': 0, 'FromImage': '', 'GridPos': [0,0]})\n",
    "            \n",
    "\n",
    "\n",
    "            # update saftey banner at bottom of output stream\n",
    "            #   green banner if safe, red if unsafe, count of people in the frame\n",
    "            \n",
    "            if _safe_cat in ['LessThanTwoPeople', 'AppropriateDistance']:\n",
    "                color = green\n",
    "                text_color = black\n",
    "            else:\n",
    "                color = red\n",
    "                text_color = white\n",
    "                \n",
    "            # Draw a color-coded banner at the bottom of the frame showing safety, num people, min distance\n",
    "            cv2.rectangle(stream.image, (0, _frame.shape[0] - 30), (_frame.shape[1], _frame.shape[0]), color, -1)\n",
    "            \n",
    "            if (_min_distance >= sd.MAX_DISTANCE):\n",
    "                banner_text = '{} people'.format(_num_total_people)\n",
    "            else:\n",
    "                banner_text = '{}, {} people, Min: {} ft'.format(_safe_cat, _num_total_people, _min_distance)\n",
    "\n",
    "            cv2.putText(stream.image, banner_text, \n",
    "                        (20, _frame.shape[0] - 10), cv2.FONT_HERSHEY_COMPLEX_SMALL, #SIMPLEX, \n",
    "                        1, text_color, 1, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "            # overlay color-coded bounding boxes on output stream, including yellow boxes for reference people\n",
    "            sdu.add_bboxes(stream.image, _likely_people, _proximity_list, self._curr_refs, _size_mask_shape)  \n",
    "            stream.image = cv2.cvtColor(stream.image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            self._frame_count += 1\n",
    "            \n",
    "            self.model.release_result(resultBatchSet)\n",
    "            outputs.video_out[i] = stream\n",
    "\n",
    "            \n",
    "        return True\n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    AwsPanoramaSD().run()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Known Issues\n",
    "----------------\n",
    "\n",
    "- Person Detector does not detect everyone in the frame\n",
    "    - Can result in False Negative SD violations\n",
    "    - Less reliable on people faraway from the camera\n",
    "    \n",
    "    \n",
    "\n",
    "- Person Detector sometime identifies non people as people\n",
    "    - This can also result in False Negative SD violations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Upload Lambda and Create Lambda Function \n",
    "----------------\n",
    "\n",
    "* A lambda is already provided and ready for use in the lambda folder (zip file)\n",
    "* Use this code snippet to upload and publish it to Lambda Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet uses boto3 to create an IAM role named LambdaBasicExecution_SD_demo with basic \n",
    "lambda execution permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "iam_client = boto3.client(\"iam\")\n",
    "\n",
    "iam_client.create_role(\n",
    "    RoleName=\"SDExecutionRole\",\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python snippet will use the resources above to create a new AWS Lambda function called SocialDistancing_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -o  ../Lambda/SocialDistanceDetection.zip  ../Lambda/lambda_function.py ../Lambda/lModelOutput.py ../Lambda/socialDistance.py ../Lambda/socialDistanceUtils.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client(\"lambda\")\n",
    "\n",
    "with open(\"../Lambda/SocialDistanceDetection.zip\", \"rb\") as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "role = iam_client.get_role(RoleName=\"SDExecutionRole\")\n",
    "response_create_function = lambda_client.create_function(\n",
    "    FunctionName=\"SocialDistancingLambda\",\n",
    "    Runtime=\"python3.7\",\n",
    "    Role=role[\"Role\"][\"Arn\"],\n",
    "    Handler=\"lambda_function.main()\",\n",
    "    Code=dict(ZipFile=zipped_code),\n",
    "    Timeout=120,\n",
    "    MemorySize=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an ARN?** : Amazon Resource Names (ARNs) uniquely identify AWS resources.\n",
    "\n",
    "The following Python snippet will publish the Lambda Function that was created above, and return an ARN with a version. \n",
    "\n",
    "This version arn can be used to go directly to the Panorama console and deploy this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.publish_version(FunctionName=\"SocialDistancingLambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the details of the lambda function that was just published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_arn = response[\"FunctionArn\"]\n",
    "function_arn_version = list(response[\"FunctionArn\"].split(\":\"))[-1]\n",
    "lambda_url = (\n",
    "    \"https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/\"\n",
    "    + response[\"FunctionName\"]\n",
    "    + \"/versions/\"\n",
    "    + response[\"Version\"]\n",
    "    + \"?tab=configuration\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### Upload tar.gz file to an S3 bucket\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    \"\"\"\n",
    "    Helper Function for Fomatting Output\n",
    "    \"\"\"\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_model_to_s3(model, bucket = 'aws-panorama-models-bucket'):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    \n",
    "    key = '../../Models/' + model\n",
    "    \n",
    "    s3.Object(bucket, model).put(Body=open(key, 'rb'))\n",
    "    \n",
    "    bucket_name = bucket\n",
    "    \n",
    "    \n",
    "    location = boto3.client('s3').get_bucket_location(Bucket='aws-panorama-models-bucket')['LocationConstraint']\n",
    "    url = \"s3://{}/{}\".format(bucket_name, model)\n",
    "    \n",
    "    printmd(\"**S3 Path** : **{}**\".format(url), color=\"black\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'ssd_512_resnet50_v1_voc.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### Next steps\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lambda is now created and published. You are now ready to deploy your model and the published lambda function, to the Panorama device\n",
    "\n",
    "The instructions to deploy are linked below\n",
    "\n",
    "[Creating Application Instructions Here](https://docs.aws.amazon.com/panorama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some helpful information about the Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Inputs**\", color=\"black\")\n",
    "print('     ')\n",
    "printmd(\"**Input Name** : **{}**\".format('data'), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format([1,3,512,512]), color=\"black\")\n",
    "printmd(\"**Order** : **{}**\".format('NCHW'), color=\"black\")\n",
    "printmd(\"**FourCC** : **{}**\".format('BGR3'), color=\"black\")\n",
    "printmd(\"**Normalize** : **{}**\".format('minmax'), color=\"black\")\n",
    "printmd(\"**Minmax range** : **{}**\".format('[0,255]'), color=\"black\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Outputs**\", color=\"black\")\n",
    "print(\"     \")\n",
    "printmd(\"**Output0**\", color=\"black\")\n",
    "printmd(\"**Output Name** : **{}**\".format(\"NMS\"), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format(\"[1,1,100,1]\"), color=\"black\")\n",
    "print(\"     \")\n",
    "printmd(\"**Output1**\", color=\"black\")\n",
    "printmd(\"**Output Name** : **{}**\".format(\"NMS1\"), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format(\"[1,1,100,1]\"), color=\"black\")\n",
    "print(\"     \")\n",
    "printmd(\"**Output2**\", color=\"black\")\n",
    "printmd(\"**Output Name** : **{}**\".format(\"NMS2\"), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format(\"[1,1,100,4]\"), color=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
