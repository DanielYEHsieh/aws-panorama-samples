{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using a Custom Tensorflow Object Detection Model with this PanoramSDK Emulator\n",
    "\n",
    "**Goal of this Notebook** :\n",
    "\n",
    "* Showcase how to use a custom build Tensorflow Object Detection model with this Emulator\n",
    "* Using the built in wrapper application that **mimics** the Panorama sdk to get inference from the model\n",
    "\n",
    "**What this Notebook accomplishes?** :\n",
    "* Detect People in a selected video\n",
    "* Draw bounding boxes around the people\n",
    "* Count the number of people detected\n",
    "* Display the count on the video frames\n",
    "\n",
    "\n",
    "**Useful Resources to aid your development**:\n",
    "* [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**CAUTION PLEASE READ BEFORE PROCEEDING** :\n",
    "\n",
    "* The panoramasdk wrapper class used in this demo is not the original Panorama sdk that is on the device image\n",
    "* The wrapper class does not reflect the capabilities of the original Panorama SDK on the device\n",
    "* Its sole purpose is to give a developer a realistic idea of the structure and signature of the sdk on the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre -Requisites**:\n",
    "* Sagemaker Instance created with the right role (Policies needed IOT, Lambda and S3, IAM Full Access) \n",
    "\n",
    "\n",
    "\n",
    "**Frames to Process**:\n",
    "\n",
    "* By default, we only process 10 frames from the video. If you want to increase this, please change this value in /panorama_sdk/panoramasdk.py and change frames_to_process = 10 to a value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "\n",
    "import jupyter_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_use = \"TownCentreXVID.avi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let the Emulator know the following\n",
    "\n",
    "1) That this is not an MXNet model zoo model  \n",
    "2) That this is a custom Object Detection model  \n",
    "3) That the framework this is built in is Tensorflow  \n",
    "4) The task at hand is object detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_utils.declare_globals({'mxnet_modelzoo_example': False, \n",
    "                               'custom_model': True, 'task':'object_detection', 'framework':'tensorflow'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf_slim\n",
    "!pip install pycocotools\n",
    "\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone -b \"v2.3.0\" https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd models/research\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v2\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "# Enable Eager Execution for Tensorflow Version < 2\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Individual Components of the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(img):\n",
    "    resized = cv2.resize(img, (300, 300))\n",
    "    x1 = np.asarray(resized)\n",
    "    #np_img = np.expand_dims(x1, 0)\n",
    "    return x1\n",
    "\n",
    "\n",
    "print('Loading Model')\n",
    "model = panoramasdk.model()\n",
    "model.open('ssd_mobilenet_v1_coco_2017_11_17', 1)\n",
    "print('Model Loaded')\n",
    "\n",
    "\n",
    "class_info = model.get_output(0)\n",
    "prob_info = model.get_output(3)\n",
    "rect_info = model.get_output(2)\n",
    "\n",
    "class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "rect_array = np.empty(rect_info.get_dims(), dtype=rect_info.get_type())\n",
    "\n",
    "dogs_image = cv2.imread('lots_of_people.jpg')\n",
    "x1 = preprocess(dogs_image)\n",
    "\n",
    "model.batch(0, x1)\n",
    "model.flush()\n",
    "\n",
    "# To implement\n",
    "resultBatchSet = model.get_result()\n",
    "\n",
    "class_batch = resultBatchSet.get(0)\n",
    "prob_batch = resultBatchSet.get(3)\n",
    "rect_batch = resultBatchSet.get(2)\n",
    "\n",
    "class_batch.get(0, class_array)\n",
    "prob_batch.get(0, prob_array)\n",
    "rect_batch.get(0, rect_array)\n",
    "\n",
    "class_data = class_array[0]\n",
    "prob_data = prob_array[0]\n",
    "rect_data = rect_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Panorama Emulator for TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "jupyter_utils.change_video_source(video_to_use)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "\n",
    "class people_counter(panoramasdk.base):\n",
    "    def interface(self):\n",
    "        return {\n",
    "            \"parameters\":\n",
    "                (\n",
    "                    (\"float\", \"threshold\", \"Detection threshold\", 0.4),\n",
    "                    (\"model\", \"people_counter\", \"Model for people counting\", \"ssd_mobilenet_v1_coco_2017_11_17\"),\n",
    "                    (\"int\", \"batch_size\", \"Model batch size\", 1),\n",
    "                    (\"float\", \"person_index\", \"person index based on dataset used\", 1)\n",
    "                ),\n",
    "            \"inputs\":\n",
    "                (\n",
    "                    (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "                ),\n",
    "            \"outputs\":\n",
    "                (\n",
    "                    (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "                )\n",
    "        }\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        try:\n",
    "            # Detection probability threshold.\n",
    "            self.threshold = parameters.threshold\n",
    "            self.frame_num = 0\n",
    "            # Number of People\n",
    "            self.number_people = 0\n",
    "            # Person Index for Model\n",
    "            self.person_index = parameters.person_index\n",
    "            # Set threshold for model\n",
    "            self.threshold = parameters.threshold\n",
    "            # Load model from the specified directory.\n",
    "            print(\"loading the model...\")\n",
    "            self.model = panoramasdk.model()\n",
    "            self.model.open(parameters.people_counter, 1)\n",
    "            print(\"model loaded\")\n",
    "            # Create input and output arrays.\n",
    "            class_info = self.model.get_output(0)\n",
    "            prob_info = self.model.get_output(3)\n",
    "            rect_info = self.model.get_output(2)\n",
    "            self.class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "            self.prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "            self.rect_array = np.empty(rect_info.get_dims(), dtype=rect_info.get_type())\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            return False\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        resized = cv2.resize(img, (300, 300))\n",
    "        x1 = np.asarray(resized)\n",
    "        #np_img = np.expand_dims(x1, 0)\n",
    "        return x1\n",
    "\n",
    "    def get_number_persons(self, class_data, prob_data):\n",
    "        # get indices of people detections in class data\n",
    "        person_indices = [i for i in range(len(class_data)) if class_data[i] == self.person_index]\n",
    "        # use these indices to filter out anything that is less than 95% threshold from prob_data\n",
    "        prob_person_indices = [i for i in person_indices if prob_data[i] >= self.threshold]\n",
    "        return prob_person_indices\n",
    "\n",
    "    def entry(self, inputs, outputs):\n",
    "        for i in range(len(inputs.video_in)):\n",
    "            stream = inputs.video_in[i]\n",
    "            person_image = stream.image\n",
    "            stream.add_label('Number : {}'.format(self.number_people), 0.8, 0.05)\n",
    "            H, W, _ = person_image.shape\n",
    "            x1 = self.preprocess(person_image)\n",
    "            # Do inference on the new frame.\n",
    "            self.model.batch(0, x1)\n",
    "            self.model.flush()\n",
    "            # Get the results.\n",
    "            resultBatchSet = self.model.get_result()\n",
    "            class_batch = resultBatchSet.get(0)\n",
    "            prob_batch = resultBatchSet.get(3)\n",
    "            rect_batch = resultBatchSet.get(2)\n",
    "            \n",
    "            class_batch.get(0, self.class_array)\n",
    "            prob_batch.get(0, self.prob_array)\n",
    "            rect_batch.get(0, self.rect_array)\n",
    "            \n",
    "            class_data = self.class_array[0]\n",
    "            prob_data = self.prob_array[0]\n",
    "            rect_data = self.rect_array[0]\n",
    "\n",
    "            person_indices = self.get_number_persons(class_data, prob_data)\n",
    "            self.number_people = len(person_indices)\n",
    "            \n",
    "            try:\n",
    "                if self.number_people > 0:\n",
    "                    for idx in person_indices:\n",
    "                        top = rect_data[idx][0]\n",
    "                        left = rect_data[idx][1]\n",
    "                        bottom = rect_data[idx][2]\n",
    "                        right = rect_data[idx][3]\n",
    "                                            \n",
    "                        stream.add_rect(left, top, right, bottom)\n",
    "                        stream.add_label('Class, thresh : {}, {}'.format(class_data[idx], prob_data[idx]), left, top)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print('exception is {}'.format(e))\n",
    "                \n",
    "\n",
    "            stream.add_label('Number : {}'.format(self.number_people), 0.8, 0.05)\n",
    "            self.model.release_result(resultBatchSet)\n",
    "            \n",
    "            outputs.video_out[i] = stream\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"run() member function is implemented in the Lambda base class. It performs Lambda initialization and enters the main loop.\"\"\"\n",
    "    \n",
    "    people_counter().run()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
