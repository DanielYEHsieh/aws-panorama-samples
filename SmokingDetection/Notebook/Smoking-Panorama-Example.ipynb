{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to create a Smoking Detection Application using AWS Panorama\n",
    "\n",
    "**Use Case** : This can be used detect if a person is smoking at places where smoking is not permitted such as gas stations, schools\n",
    "\n",
    "**Goal of this Notebook** :\n",
    "\n",
    "* Aid an Panorama developer prototype their application before creating the AWS Lambda for Panorama\n",
    "* Creating a Panorama SDK\n",
    "application in the same structure as the AWS Lambda for Panorama\n",
    "* Create and Deploy the AWS Lambda for Panorama from this notebook\n",
    "\n",
    "**What this Notebook accomplishes?** :\n",
    "* Detect Smoking using Kinetics 400 Model using MXNet\n",
    "* Show the structure of the AWS Lambda for Panorama and replicate it using MXNet\n",
    "* Create and publish the included AWS Lambda for Panorama directly to the AWS Lambda service\n",
    "\n",
    "\n",
    "\n",
    "**Pre -Requisites**:\n",
    "* Sagemaker Instance created with the right role (Policies needed IOT, Lambda and S3, IAM Full Access) ( Add Doc here)\n",
    "\n",
    "\n",
    "**Frames to Process**:\n",
    "\n",
    "* By default, we only process 10 frames from the video. If you want to increase this, please change this value in /panorama_sdk/panoramasdk.py and change frames_to_process = 10 to a value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_use = \"smoking.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import boto3\n",
    "import json\n",
    "from mxnet import nd\n",
    "from gluoncv.model_zoo import get_model\n",
    "import numpy as np\n",
    "from gluoncv import utils\n",
    "from IPython.display import Video\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    \"\"\"\n",
    "    Helper Function for Fomatting Output\n",
    "    \"\"\"\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Code Walk Through\n",
    "\n",
    "This step walks through using the MXNet model to get inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model** : resnet101_v1b_kinetics400  \n",
    "**Kinetics400** : This models is trained on [Kinetics400](https://deepmind.com/research/open-source/kinetics), a large-scale, high-quality dataset of URL links to approximately 650,000 video clips that covers 400 human action classes, including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging.   \n",
    "**arXiv** :[The Kinetics Human Action Video Dataset](https://arxiv.org/abs/1705.06950)  \n",
    "**Model Input Size** : 224 x 224  \n",
    "**Model Output** : 400 Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.1. Loading the model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_detection_model_name = 'resnet101_v1b_kinetics400'\n",
    "action_detection_model = get_model(action_detection_model_name, nclass=400, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2. Loading the video that we will do inference on**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'smoking.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.3. Preprocessing function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size):\n",
    "    \"\"\"\n",
    "    # Performing transformations for the video clip / camera feed. This transformation function\n",
    "    # does three things: center crop the image to 224x224 in size,\n",
    "    # transpose it and normalize with\n",
    "    # mean and standard deviation calculated across all frames.\n",
    "\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(img, (size, size))\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]  # RGB\n",
    "    std = [0.229, 0.224, 0.225]  # RGB\n",
    "\n",
    "    img = resized.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "    img_a = img[:, :, 0]\n",
    "    img_b = img[:, :, 1]\n",
    "    img_c = img[:, :, 2]\n",
    "\n",
    "    # Extracting single channels from 3 channel image\n",
    "    # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "\n",
    "    # normalizing per channel data:\n",
    "    img_a = (img_a - mean[0]) / std[0]\n",
    "    img_b = (img_b - mean[1]) / std[1]\n",
    "    img_c = (img_c - mean[2]) / std[2]\n",
    "\n",
    "    # putting the 3 channels back together:\n",
    "    x1 = [[[], [], []]]\n",
    "    x1[0][0] = img_a\n",
    "    x1[0][1] = img_b\n",
    "    x1[0][2] = img_c\n",
    "\n",
    "    x1 = np.asarray(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4. Using the model to do inference\n",
    "\n",
    "The below example code does inference only on the first 180 frames. The code also visualizes the top 3 actions detected at each frame\n",
    "  \n",
    "**Note :** The actual / meaningful predictions start at around Frame 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the video using opencv\n",
    "cap = cv2.VideoCapture(inputs)\n",
    "frame_number = 0\n",
    "inference_on_number_of_frames = 180\n",
    "\n",
    "# Looping thru the video\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    # Increment Frame Number\n",
    "    frame_number += 1\n",
    "    \n",
    "    (grabbed, frame) = cap.read()\n",
    "\n",
    "    # Preprocess Frame. We resize to 224 x 224\n",
    "    prep_frame = preprocess(frame, 224)\n",
    "\n",
    "    # Predict\n",
    "    pred = action_detection_model(nd.array(prep_frame))\n",
    "    \n",
    "    # Get Classes\n",
    "    classes = action_detection_model.classes\n",
    "    \n",
    "    # If you want to look at the classes, uncomment the next print statement. \n",
    "    # Warning: really large list will be printed. \n",
    "    #print(classes)\n",
    "    \n",
    "    # Get Top n classes predicted\n",
    "    topK = 3\n",
    "    ind = nd.topk(pred, k=topK)[0].astype('int')\n",
    "    topKlist = []\n",
    "\n",
    "    # Collect the top n Classes Predicted and print on screen\n",
    "    for i in range(topK):\n",
    "        a = 25 + 25 * i\n",
    "        class_pred = classes[ind[i].asscalar()]\n",
    "        cv2.putText(frame, 'Action {}: {}'.format(i + 1,class_pred), (10, a), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, 'frame : {}'.format(frame_number), (10, a + 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Output the frame\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    \n",
    "    # Clear output\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # observe the keypress by the user\n",
    "    keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the user pressed \"q\", then stop looping\n",
    "    if keypress == ord(\"q\") or frame_number == inference_on_number_of_frames:\n",
    "        break\n",
    "\n",
    "# free up memory\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Structuring the lambda function\n",
    "\n",
    "\n",
    "* Steps 2.1-2.4 showed how to use the Action Recognition model to do inference. \n",
    "\n",
    "* This step will introduce the structure of the Panorama Lambda. \n",
    "\n",
    "* This step also walks through the code to understand how the Lambda structure can be created. \n",
    "\n",
    "**Note**: \n",
    "\n",
    "* The code structure below is as close to the lambda structure as possible, but is **NOT** the actual Lambda.\n",
    "\n",
    "* To see and compare the actual lambda to this code, please open the .py file in the Lambda folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Panorama Lambda function has the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lambda skeleton\n",
    "\n",
    "\n",
    "class SmokingDetection(object):\n",
    "    def interface(self):\n",
    "        # defines the parameters that interface with other services from Panorama\n",
    "        return\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        # defines the attributes such as arrays and model objects that will be used in the application\n",
    "        return\n",
    "\n",
    "    def entry(self, inputs, outputs):\n",
    "        # defines the application logic responsible for predicting using the inputs and handles what to do\n",
    "        # with the outputs\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "\n",
    "import jupyter_utils\n",
    "\n",
    "jupyter_utils.change_video_source(video_to_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import boto3\n",
    "from classes import load_classes\n",
    "import json\n",
    "\n",
    "\n",
    "class ActionDetection(panoramasdk.base):\n",
    "    \n",
    "    def interface(self):\n",
    "        return {\n",
    "            \"parameters\":\n",
    "            (\n",
    "                (\"model\", \"action_detection\", \"Model for detecting_action\", \"resnet101_v1b_kinetics400\"),\n",
    "            ),\n",
    "            \"inputs\":\n",
    "            (\n",
    "                (\"media[]\", \"video_in\", \"Camera input stream\"),\n",
    "            ),\n",
    "            \"outputs\":\n",
    "            (\n",
    "                (\"media[video_in]\", \"video_out\", \"Camera output stream\"),\n",
    "            )\n",
    "            \n",
    "        }\n",
    "        \n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        \n",
    "        try:\n",
    "            # Frame Number\n",
    "            self.frame_num = 0\n",
    "\n",
    "            # Load model from the specified directory\n",
    "            print(\"loading model\")\n",
    "            self.action_detection_model = panoramasdk.model()\n",
    "            self.action_detection_model.open(parameters.action_detection, 1)\n",
    "            print(\"model loaded\")\n",
    "\n",
    "            # Panorama SDK specific declarations\n",
    "            self.class_name_list = []\n",
    "            self.class_prob_list = []\n",
    "            class_info = self.action_detection_model.get_output(0)\n",
    "            self.class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "                        \n",
    "            # topK \n",
    "            self.topK = 5\n",
    "            \n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            return False\n",
    "    \n",
    "    def preprocess(self, img, size):\n",
    "        \n",
    "        resized = cv2.resize(img, (size, size))\n",
    "        mean = [0.485, 0.456, 0.406]  # RGB\n",
    "        std = [0.229, 0.224, 0.225]  # RGB\n",
    "        \n",
    "        img = resized.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "        img_a = img[:, :, 0]\n",
    "        img_b = img[:, :, 1]\n",
    "        img_c = img[:, :, 2]\n",
    "        \n",
    "        # Extracting single channels from 3 channel image\n",
    "        # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "        # normalizing per channel data:\n",
    "        img_a = (img_a - mean[0]) / std[0]\n",
    "        img_b = (img_b - mean[1]) / std[1]\n",
    "        img_c = (img_c - mean[2]) / std[2]\n",
    "        \n",
    "        # putting the 3 channels back together:\n",
    "        x1 = [[[], [], []]]\n",
    "        x1[0][0] = img_a\n",
    "        x1[0][1] = img_b\n",
    "        x1[0][2] = img_c\n",
    "        x1 = np.asarray(x1)\n",
    "        \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "    def entry(self, inputs, outputs):\n",
    "        \n",
    "        for i in range(len(inputs.video_in)):\n",
    "            stream = inputs.video_in[i]\n",
    "            \n",
    "            self.frame_num += 1\n",
    "            \n",
    "            image_input = stream.image\n",
    "            \n",
    "            # Pre Process Frame\n",
    "            prep_frame = self.preprocess(image_input, 224)\n",
    "\n",
    "            # Predict\n",
    "            self.action_detection_model.batch(0, prep_frame)\n",
    "            self.action_detection_model.flush()\n",
    "\n",
    "            # Get the results.\n",
    "            resultBatchSet = self.action_detection_model.get_result()\n",
    "                        \n",
    "            class_batch = resultBatchSet.get(0)\n",
    "            class_batch.get(0, self.class_array)\n",
    "            class_data = self.class_array[0]\n",
    "            \n",
    "            # Load Classes\n",
    "            classes = load_classes()\n",
    "            \n",
    "            # declare topKlist\n",
    "            topKlist = []\n",
    "            \n",
    "            # Collect the Top 10 Classes\n",
    "            sorted_vals = sorted(((value,index) for index, value in enumerate(class_data)), reverse=True)\n",
    "            ind = [d for (c,d) in sorted_vals][0:self.topK]\n",
    "            \n",
    "            \n",
    "            x1 = 0.05\n",
    "            y1 = 0.1\n",
    "            for z in range(len(ind)):\n",
    "                class_name = classes[ind[z]]\n",
    "                topKlist.append(class_name)\n",
    "                stream.add_label(class_name, x1, y1 + z*0.08)\n",
    "                \n",
    "                \n",
    "    \n",
    "            # Top k action list to MQTT message\n",
    "            #response = iot.publish(topic='SmokingDetection',qos=1,payload=json.dumps({\"results\":topKlist,\"message\":self.message}))\n",
    "        \n",
    "            self.action_detection_model.release_result(resultBatchSet)\n",
    "            outputs.video_out[i] = stream\n",
    "\n",
    "        return True\n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ActionDetection().run()\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Lambda and Create Lambda Function\n",
    "\n",
    "* A lambda is already provided and ready for use in the lambda folder (zip file)\n",
    "* Use this code snippet to upload and publish it to Lambda Service\n",
    "\n",
    "### **4.1 AWS CLI credentials:**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Create Role**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam_client = boto3.client(\"iam\")\n",
    "\n",
    "iam_client.create_role(\n",
    "    RoleName=\"SmokingDetectionExecutionRole\",\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet uses boto3 to create an IAM role named LambdaBasicExecutionSmoke with basic lambda execution permissions.\n",
    "\n",
    "The following Python snippet will use all of the resources above to create a new AWS Lambda function called ActionDetectionLambdaDemo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -o  ../Lambda/SmokingDetectionLambda.zip  ../Lambda/action_detection.py ../Lambda/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "with open('../Lambda/SmokingDetectionLambda.zip', 'rb') as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "role = iam_client.get_role(RoleName='SmokingDetectionExecutionRole')\n",
    "response_create_function = lambda_client.create_function(\n",
    "  FunctionName='SmokingDetectionLambda',\n",
    "  Runtime='python3.7',\n",
    "  Role=role['Role']['Arn'],\n",
    "  Handler='action_detection.main',\n",
    "  Code=dict(ZipFile=zipped_code),\n",
    "  Timeout=120, \n",
    "  MemorySize=2048,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python snippet will publish the Lambda Function we created above and return an ARN with a version. We can then use this version arn to go directly to the Panorama console and deploy our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish Lambda\n",
    "response = lambda_client.publish_version(\n",
    "      FunctionName='SmokingDetectionLambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Print latest version number \n",
    "\n",
    "What is an ARN? : Amazon Resource Names (ARNs) uniquely identify AWS resources.\n",
    "\n",
    "The following Python snippet will publish the Lambda Function we created above, and return an ARN with a version.\n",
    "\n",
    "This version arn can be used to go directly to the Panorama console and deploy this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_arn = response['FunctionArn']\n",
    "function_arn_version = list(response['FunctionArn'].split(':'))[-1]\n",
    "lambda_url = 'https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/' + response['FunctionName'] + '/versions/' + response['Version'] + '?tab=configuration'\n",
    "\n",
    "printmd(\"**Function Arn** : **{}**\".format(function_arn) , color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version) , color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url) , color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Upload Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_model_to_s3(model, bucket = 'aws-panorama-models-bucket'):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    \n",
    "    key = '../../Models/' + model\n",
    "    \n",
    "    s3.Object(bucket, model).put(Body=open(key, 'rb'))\n",
    "    \n",
    "    bucket_name = bucket\n",
    "    \n",
    "    \n",
    "    location = boto3.client('s3').get_bucket_location(Bucket='aws-panorama-models-bucket')['LocationConstraint']\n",
    "    url = \"s3://{}/{}\".format(bucket_name, model)\n",
    "    \n",
    "    printmd(\"**S3 Path** : **{}**\".format(url), color=\"black\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'resnet101_v1b_kinetics400.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Deploy the Application\n",
    "\n",
    "Now that our Lambda is created and published, you are ready to deploy your model and the lambda function you created here, to the Panorama device\n",
    "\n",
    "[Creating Application Instructions Here](https://alpha-docs-aws.amazon.com/omni/latest/devguide/omni-welcome.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful information about Lambda and Model for deploying through the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")\n",
    "\n",
    "printmd(\"**Inputs for resnet101_v1b_kinetics400 Model**\", color=\"black\")\n",
    "print('     ')\n",
    "printmd(\"**Input Name** : **{}**\".format('data'), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format([1,3,224, 224]), color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
