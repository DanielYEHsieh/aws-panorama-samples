{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to create a Fall Detection Application using Panoramasdk\n",
    "\n",
    "#### Goal of this Notebook :\n",
    "* Aid an Panorama SDK developer prototype their application before creating the PanoramaSDK Lambda\n",
    "* Creating an MXNET application in the same structure as the PanoramaSDK Lambda\n",
    "* Create and Deploy the PanoramaSDK Lambda from this notebook\n",
    "\n",
    "#### Application Use Case :\n",
    "* Detect Fall with `simplepose`, a pose estimation model\n",
    "* Time the fall and display the fall metrics\n",
    "\n",
    "#### Useful Resources to aid your development :\n",
    "* [AWS Panorama Documentation](https://docs.aws.amazon.com/panorama/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a video from [URD Dataset](http://le2i.cnrs.fr/Fall-detection-Dataset?lang=fr) , one of the common datasets used for training a Fall Detection model.  <br>\n",
    "**Note:**  Use conda_mxnet_p36 kernel to run this notebook and install GluonCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, clear_output, Markdown, display\n",
    "\n",
    "# notebook helper script\n",
    "from nb_utils import get_frames, get_video_stats, get_predictions, to_mx, to_np\n",
    "\n",
    "# lambda helper script\n",
    "from notebook_utils import (\n",
    "    preprocess,\n",
    "    detector_to_simple_pose,\n",
    "    heatmap_to_coord,\n",
    "    update_x,\n",
    "    update_y,\n",
    "    reset_tracker,\n",
    "    reset_counts,\n",
    ")\n",
    "\n",
    "video_path = \"sample_video.mp4\"\n",
    "video_stats = get_video_stats(video_path)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Observations on fall:\n",
    "\n",
    "- There is a steep change in y-coordinates of human-body keypoints\n",
    "- The ankle to shoulder y-coordinate distance reduces as the person falls\n",
    "- The head/upper body moves at high velocity downwards (y-axis) during a fall compared to regular walking action\n",
    "\n",
    "To build a simple fall detector, we need to be able to detect and track presence of keypoints from frame to frame and incorporate custom post processing logic. <br>\n",
    "The potential models that can help achieve this are explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keypoint Detection in a top-down approach is a two step process:\n",
    "\n",
    "- Step 1: Indentify the presence of \"person\" in the frame with a bounding box(bbox) using an Object Detection model.\n",
    "- Step 2: Predict the keypoints of the person from the bbox image using a Pose Estimation model.\n",
    "\n",
    "To simplify, let's start with a single person fall detection algorithm. \n",
    "\n",
    "GluonCV Model Zoo (https://gluon-cv.mxnet.io/model_zoo/) offers a range pretrained models with several architecture and backbone network combinations to choose from, for various latency and performance considerations. <br>\n",
    "For this application `ssd_512_resnet50_v1_coco` will be used for person detection and `simple_pose_resnet50_v1d` for human-body keypoint detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# dependencies for model\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv import data, utils\n",
    "\n",
    "# lambda helper script\n",
    "from notebook_utils import (\n",
    "    preprocess,\n",
    "    detector_to_simple_pose,\n",
    "    heatmap_to_coord,\n",
    "    update_x,\n",
    "    update_y,\n",
    "    reset_tracker,\n",
    "    reset_counts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image from video\n",
    "sample_frames = get_frames(video_path, frame_nums=range(0, video_stats[\"total_frames\"]))\n",
    "sample_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed image size for model\n",
    "img_size = (512, 512)\n",
    "# Object Detection confidence threshold\n",
    "conf_thresh = 0.1\n",
    "# Valid bbox size for person detection\n",
    "box_size_thresh = (75, 75)\n",
    "\n",
    "# Load pre-trained object detection model and reset classes to get only predictions for person\n",
    "detector_model = get_model(name=\"ssd_512_resnet50_v1_coco\", pretrained=True)\n",
    "detector_model.reset_class([\"person\"], reuse_weights=[\"person\"])\n",
    "\n",
    "# Load pose estimation model\n",
    "pose_model = get_model(name=\"simple_pose_resnet50_v1d\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Person Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, orig_img = preprocess(sample_frames[200], img_size)\n",
    "x, orig_img = mx.nd.array(x), mx.nd.array(orig_img)\n",
    "class_ids, scores, bboxes = detector_model(x)\n",
    "\n",
    "class_ids.shape, scores.shape, bboxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_ids - top 100 classes of objects in the image. The class_id for `person` class is 0 <br>\n",
    "scores - confidence scores for the class predictions <br>\n",
    "bboxes - the top-left, bottom-right (x,y) coordinates bounding box that surrounds the object of interest<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top predictions\n",
    "print(\"Class id : \", class_ids[0][0].asscalar())\n",
    "print(\"Confidence score : \", scores[0][0].asscalar())\n",
    "print(\"Bounding box prediction : \", bboxes[0][0].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = utils.viz.plot_bbox(\n",
    "    orig_img, bboxes[0], scores[0], class_ids[0], class_names=detector_model.classes\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining valid object detection prediction:**\n",
    "\n",
    "* Filter only the top-most confindent prediction from object detection model (single person tracking) and check for confidence $\\gt$  `conf_thresh`\n",
    "* Remove false positive predictions from the surrounding objects which has bounding box dimensions (w,h) less than a certain `box_size_thresh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Pose Estimation/ Key point detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox coordinates of most confident prediction\n",
    "x_min, y_min, x_max, y_max = (\n",
    "    bboxes[0][0][0],\n",
    "    bboxes[0][0][1],\n",
    "    bboxes[0][0][2],\n",
    "    bboxes[0][0][3],\n",
    ")\n",
    "w, h = (x_max - x_min).asscalar(), (y_max - y_min).asscalar()\n",
    "\n",
    "# do pose estimation only for valid preidctions\n",
    "if (\n",
    "    (scores[:, 0:1, :][0][0].asscalar() > conf_thresh)\n",
    "    and w > box_size_thresh[0]\n",
    "    and h > box_size_thresh[1]\n",
    "):\n",
    "\n",
    "    # crop the image with bbox prediction from detector_model\n",
    "    # process it for pose_model\n",
    "    orig_img, class_ids, scores, bboxes = (\n",
    "        to_np(orig_img),\n",
    "        to_np(class_ids),\n",
    "        to_np(scores),\n",
    "        to_np(bboxes),\n",
    "    )\n",
    "    pose_input, upscale_bbox = detector_to_simple_pose(\n",
    "        orig_img,\n",
    "        class_ids[:, 0:1, :],\n",
    "        scores[:, 0:1, :],\n",
    "        bboxes[:, 0:1, :],\n",
    "        thr=conf_thresh,\n",
    "        person_id=0,\n",
    "    )\n",
    "\n",
    "    pose_input, upscale_bbox = to_mx(pose_input), to_mx(upscale_bbox)\n",
    "\n",
    "    # Get keypoint heatmap predictions\n",
    "    predicted_heatmap = pose_model(pose_input)\n",
    "    predicted_heatmap, upscale_bbox = to_np(predicted_heatmap), to_np(upscale_bbox)\n",
    "\n",
    "    # Convert the heat map to (x,y) coordinate space in the frame and return confidence scores for 17 keypoints\n",
    "    pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SimplePose model input image dimension  :\", pose_input.shape)\n",
    "print(\"SimplePose model output tensor dimension  :\", predicted_heatmap.shape)\n",
    "print(\"Processed prediction coordinates dimension  :\", pred_coords.shape)\n",
    "print(\"Processed confidence scores dimension  :\", confidence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimplePose model predicts the coordinates and confidence scores for 17 keypoints in the body in the order: <br>\n",
    "\n",
    "[ nose, left_eye, right_eye, left_ear, right_ear, left_shoulder, right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist, left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Track metrics for fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_utils import get_frames, get_video_stats, get_predictions, to_mx, to_np\n",
    "\n",
    "\n",
    "frame_num = 290\n",
    "print(\"Frame number : \", frame_num)\n",
    "get_predictions(\n",
    "    sample_frames[frame_num],\n",
    "    detector_model,\n",
    "    pose_model,\n",
    "    img_size,\n",
    "    person_id=0,\n",
    "    visualize=True,\n",
    ")\n",
    "\n",
    "\n",
    "frame_num = 400\n",
    "print(\"Frame number : \", frame_num)\n",
    "get_predictions(\n",
    "    sample_frames[frame_num],\n",
    "    detector_model,\n",
    "    pose_model,\n",
    "    img_size,\n",
    "    person_id=0,\n",
    "    visualize=True,\n",
    ")\n",
    "\n",
    "\n",
    "frame_num = 633\n",
    "print(\"Frame number : \", frame_num)\n",
    "get_predictions(\n",
    "    sample_frames[frame_num],\n",
    "    detector_model,\n",
    "    pose_model,\n",
    "    img_size,\n",
    "    person_id=0,\n",
    "    visualize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ankle shoulder distance reduces from 154 pixels to 26 pixels as the person falls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define variables to track ankle to shoulder distance.\n",
    "\n",
    "* `xpart_tracker` - is a dictionary that tracks the x coordinates of keypoints of shoulders, ankles in numpy array\n",
    "* `ypart_tracker` - is a dictionary that tracks the y coordinates of keypoints of shoulders, ankles in numpy array\n",
    "* ankle to shoulder distance will be measured by `ypart_tracker['shdr']-ypart_tracker['anks']`.\n",
    "\n",
    "The fall detection assessment is carried out in function `fall_detection`. <br>\n",
    "**Fall** is declared when ankle to shoulder distance `anks-shdr` distance along y-axis reduces below `anks-shdr-thresh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Combine all rules\n",
    "\n",
    "The below example code does inference only on the mid 100 frames and flags a fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fall_detection(ypart_tracker, anks_shdr_thresh, dist_hist=50, dist_count=5):\n",
    "    \"\"\"\n",
    "    :param ypart_tracker: y points tracker\n",
    "    :param anks_shdr_thresh: threshold for distance\n",
    "    :param dist_hist: number of points in history to consider\n",
    "    :param dist_count: threshold for number of occurrences of low anks-shdr distance\n",
    "    :return: Fall result\n",
    "    \"\"\"\n",
    "    dist_cndt = np.sum(ypart_tracker[\"anks-shdr\"][-dist_hist:] <= anks_shdr_thresh)\n",
    "\n",
    "    if dist_cndt > dist_count:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs # folder to save fall-frame\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('logs/')\n",
    "\n",
    "except Exception as e:\n",
    "    print('{}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for inference and tracking\n",
    "\n",
    "xpart_tracker, ypart_tracker = reset_tracker()  # Keypoint tracking list\n",
    "(\n",
    "    frame_num,\n",
    "    frame_prev,\n",
    "    frame_curr,\n",
    "    zero_dets,\n",
    ") = reset_counts()  # Frame numbers tracking\n",
    "\n",
    "# preprocessing\n",
    "img_size = (512, 512)\n",
    "\n",
    "# Person detection variables\n",
    "conf_thresh = 0.1\n",
    "person_id = 0\n",
    "box_size_thresh = (75, 75)\n",
    "\n",
    "# Fall variables\n",
    "anks_shdr_thresh = 50  # Threshold for ankle shoulder distance\n",
    "dist_hist = 50  # Number of frames in the past to evaluate distance\n",
    "dist_count = 5  # Number of frames where ankle shoulder distance goes below dist_hist\n",
    "\n",
    "# Tracking variables\n",
    "fall_idx = -1  # Last fall frame number\n",
    "min_non_dets = 20  # count of minimum number of non-detection frames\n",
    "fall_interval = 1000  # number of frames to consider next fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the video using opencv\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Looping through the video\n",
    "while (cap.isOpened()) and i < video_stats[\"total_frames\"]:\n",
    "\n",
    "    (fetch, frame) = cap.read()\n",
    "    # Convert to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    i += 1\n",
    "\n",
    "    if i < 350 or i > 450:\n",
    "        continue\n",
    "\n",
    "    x, orig_img = preprocess(frame, img_size)\n",
    "    x, orig_img = mx.nd.array(x), mx.nd.array(orig_img)\n",
    "\n",
    "    # Detect person\n",
    "    box_ids, scores, bboxes = detector_model(x)\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        bboxes[0][0][0],\n",
    "        bboxes[0][0][1],\n",
    "        bboxes[0][0][2],\n",
    "        bboxes[0][0][3],\n",
    "    )\n",
    "    w, h = (x_max - x_min).asscalar(), (y_max - y_min).asscalar()\n",
    "\n",
    "    # Check for minimum person detection confidence and bbox dimension of person\n",
    "    if (\n",
    "        (scores[:, 0:1, :][0][0].asscalar() > conf_thresh)\n",
    "        and w > box_size_thresh[0]\n",
    "        and h > box_size_thresh[1]\n",
    "    ):\n",
    "\n",
    "        orig_img, box_ids, scores, bboxes = (\n",
    "            to_np(orig_img),\n",
    "            to_np(box_ids),\n",
    "            to_np(scores),\n",
    "            to_np(bboxes),\n",
    "        )\n",
    "\n",
    "        # get pose keypoints\n",
    "        try:\n",
    "            # Crop the bbox area from detector output from original image, transform it for pose model\n",
    "            pose_input, upscale_bbox = detector_to_simple_pose(\n",
    "                orig_img,\n",
    "                box_ids[:, 0:1, :],\n",
    "                scores[:, 0:1, :],\n",
    "                bboxes[:, 0:1, :],\n",
    "                thr=conf_thresh,\n",
    "                person_id=person_id,\n",
    "            )\n",
    "\n",
    "            pose_input, upscale_bbox = to_mx(pose_input), to_mx(upscale_bbox)\n",
    "            # process pose model output to get key point coordinates\n",
    "            predicted_heatmap = pose_model(pose_input)\n",
    "            predicted_heatmap, upscale_bbox = to_np(predicted_heatmap), to_np(\n",
    "                upscale_bbox\n",
    "            )\n",
    "            pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)\n",
    "\n",
    "            # Append current prediction to previous\n",
    "            xpart_tracker = update_x(pred_coords[0][:, 0], xpart_tracker)\n",
    "            ypart_tracker = update_y(pred_coords[0][:, 1], ypart_tracker)\n",
    "\n",
    "            result = fall_detection(\n",
    "                ypart_tracker, anks_shdr_thresh, dist_hist, dist_count\n",
    "            )\n",
    "\n",
    "            if result:\n",
    "                # Flag next fall after fall_interval frames\n",
    "                if fall_idx == -1 or (i - fall_idx) >= (fall_interval):\n",
    "                    time_stamp = time.strftime(\n",
    "                        \"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())\n",
    "                    )\n",
    "                    print(f\"Fall Detected at {time_stamp} {i}, saving image to logs.\")\n",
    "                    fall_idx = i\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # Reset tracker if no person is detected for more than `min_non_dets` continuous frames\n",
    "        if zero_dets > min_non_dets:\n",
    "            xpart_tracker, ypart_tracker = reset_tracker()\n",
    "            frame_num, frame_prev, frame_curr, zero_dets = reset_counts()\n",
    "            continue\n",
    "        # Track consecutive non detections\n",
    "        frame_prev, frame_curr = frame_curr, frame_num\n",
    "        if frame_curr - frame_prev == 1:\n",
    "            zero_dets += 1\n",
    "        else:\n",
    "            zero_dets = 0\n",
    "\n",
    "    frame_num += 1\n",
    "    dist = round((ypart_tracker[\"anks\"][-1] - ypart_tracker[\"shdr\"][-1]), 2)\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Fall Detected in frame : {}\".format(max(-1, fall_idx-1)),\n",
    "        (10, 50),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.8,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Ankle Shoulder Distance : {}\".format(dist),\n",
    "        (10, 75),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.8,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "    \n",
    "    # Save the fall image\n",
    "    if fall_idx == i: plt.imsave(\"logs/fall.jpg\", frame)\n",
    "        \n",
    "    # Output the frame\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "\n",
    "    # Clear output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # observe the keypress by the user\n",
    "    keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the user pressed \"q\", then stop looping\n",
    "    if keypress == ord(\"q\"):\n",
    "        breakq\n",
    "\n",
    "# free up memory\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above 4 steps showed how to process inference from pose estimation model to frame rules for fall detection. <br>\n",
    "In order to deploy \"Fall Detection application\", the code above has to be converted into a structured lambda function.<br> The generic template is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda skeleton\n",
    "class FallDetection(object):\n",
    "    def interface(self):\n",
    "        # defines the parameters that interface with other services from Panorama\n",
    "        return\n",
    "\n",
    "    def init(self, parameters, inputs, outputs):\n",
    "        # defines the attributes such as arrays and model objects that will be used in the application\n",
    "        return\n",
    "\n",
    "    def entry(self, inputs, outputs):\n",
    "        # defines the application logic responsible for predicting using the inputs and handles what to do\n",
    "        # with the outputs\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below closely follows the lambda function script in Lambda/FallDetection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath(os.path.join(os.path.dirname(\"panorama_sdk\"), '../..'))\n",
    "sys.path.insert(1, path + '/panorama_sdk')\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "import utils as lambda_utils\n",
    "\n",
    "\n",
    "from gluoncv import model_zoo, data, utils\n",
    "from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading models in Python Panoramasdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "\n",
    "pose_model = panoramasdk.model()\n",
    "pose_model.open('simple_pose_resnet50_v1d', 1)\n",
    "\n",
    "\n",
    "detector_model = panoramasdk.model()\n",
    "detector_model.open('ssd_512_resnet50_v1_voc', 1)\n",
    "\n",
    "\n",
    "\n",
    "# Create input and output arrays.\n",
    "class_info = detector_model.get_output(0)\n",
    "prob_info = detector_model.get_output(1)\n",
    "rect_info = detector_model.get_output(2)\n",
    "\n",
    "class_array = np.empty(class_info.get_dims(), dtype=class_info.get_type())\n",
    "prob_array = np.empty(prob_info.get_dims(), dtype=prob_info.get_type())\n",
    "rect_array = np.empty(rect_info.get_dims(), dtype=rect_info.get_type())\n",
    "\n",
    "\n",
    "heatmap_info = pose_model.get_output(0)\n",
    "heatmaps_array = np.empty(heatmap_info.get_dims(), dtype=heatmap_info.get_type())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size):\n",
    "\n",
    "    resized = cv2.resize(img, (size, size))\n",
    "    mean = [0.485, 0.456, 0.406]  # RGB\n",
    "    std = [0.229, 0.224, 0.225]  # RGB\n",
    "\n",
    "    img = resized.astype(np.float32) / 255.  # converting array of ints to floats\n",
    "    img_a = img[:, :, 0]\n",
    "    img_b = img[:, :, 1]\n",
    "    img_c = img[:, :, 2]\n",
    "\n",
    "    # Extracting single channels from 3 channel image\n",
    "    # The above code could also be replaced with cv2.split(img) << which will return 3 numpy arrays (using opencv)\n",
    "    # normalizing per channel data:\n",
    "    img_a = (img_a - mean[0]) / std[0]\n",
    "    img_b = (img_b - mean[1]) / std[1]\n",
    "    img_c = (img_c - mean[2]) / std[2]\n",
    "\n",
    "    # putting the 3 channels back together:\n",
    "    x1 = [[[], [], []]]\n",
    "    x1[0][0] = img_a\n",
    "    x1[0][1] = img_b\n",
    "    x1[0][2] = img_c\n",
    "    x1 = np.asarray(x1)\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n",
    "                          'gluoncv/pose/soccer.png?raw=true',\n",
    "                          path='soccer.png')\n",
    "x, img = data.transforms.presets.ssd.load_test(im_fname, short=512)\n",
    "print('Shape of pre-processed image:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get person detection and pose detection in Panoramasdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Frame\n",
    "x1 = preprocess(img, 512)\n",
    "\n",
    "# Do inference on the new frame.\n",
    "detector_model.batch(0, x1)\n",
    "detector_model.flush()\n",
    "\n",
    "# Get the results.\n",
    "resultBatchSet = detector_model.get_result()\n",
    "\n",
    "class_batch = resultBatchSet.get(0)\n",
    "prob_batch = resultBatchSet.get(1)\n",
    "rect_batch = resultBatchSet.get(2)\n",
    "\n",
    "class_batch.get(0, class_array)\n",
    "prob_batch.get(1, prob_array)\n",
    "rect_batch.get(2, rect_array)\n",
    "\n",
    "class_data = class_array[0]\n",
    "prob_data = prob_array[0]\n",
    "rect_data = rect_array[0]\n",
    "\n",
    "\n",
    "\n",
    "pose_input, upscale_bbox = lambda_utils.detector_to_simple_pose(img,  class_array, prob_array, rect_array, person_index = 14.0)\n",
    "\n",
    "# Do inference on the new frame.\n",
    "pose_model.batch(0, pose_input)\n",
    "pose_model.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PresultBatchSet = pose_model.get_result()\n",
    "heatmaps_batch = PresultBatchSet.get(0)\n",
    "heatmaps_batch.get(0, heatmaps_array)\n",
    "predicted_heatmap = heatmaps_array\n",
    "pose_model.release_result(PresultBatchSet)\n",
    "\n",
    "\n",
    "\n",
    "# # process pose model output to get key point coordinates\n",
    "pred_coords, confidence = lambda_utils.heatmap_to_coord(predicted_heatmap, upscale_bbox)\n",
    "pred_coords = np.round(pred_coords, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = utils.viz.plot_keypoints(img, pred_coords, confidence,\n",
    "                              class_array, rect_array, prob_array,\n",
    "                              box_thresh=0.5, keypoint_thresh=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stucturing the Lambda for the Fall Detection Usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gluoncv dependencies for model, visualization\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv import data, utils\n",
    "\n",
    "# lambda helper script\n",
    "from notebook_utils import (\n",
    "    reset_counts,\n",
    "    reset_tracker,\n",
    "    update_x,\n",
    "    preprocess,\n",
    "    update_y,\n",
    "    fall_detection,\n",
    "    detector_to_simple_pose,\n",
    "    heatmap_to_coord,\n",
    ")\n",
    "\n",
    "# notebook specific helper script\n",
    "from nb_utils import get_video_stats, get_frames, to_np, to_mx\n",
    "from IPython.display import clear_output, Markdown, display\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"sample_video.mp4\"\n",
    "video_stats = get_video_stats(video_path, verbose=False)\n",
    "\n",
    "model_params = {\n",
    "    \"detector_name\": \"ssd_512_resnet50_v1_coco\",\n",
    "    \"pose_name\": \"simple_pose_resnet50_v1d\",\n",
    "    \"pretrained\": True,\n",
    "    \"person_id\": 0,\n",
    "    \"conf_thresh\": 0.1,\n",
    "}\n",
    "\n",
    "input_params = {\n",
    "    \"video_name\": video_path,\n",
    "    \"img_size\": (512, 512),\n",
    "}\n",
    "\n",
    "fall_constraints = {\n",
    "    \"box_size_thresh\": (75, 75),\n",
    "    \"min_non_dets\": 20,\n",
    "    \"dist_hist\": 50,\n",
    "    \"dist_count\": 5,\n",
    "    \"anks_shdr_thresh\": 50,\n",
    "    \"fall_interval\": 1000,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class FallDetection:\n",
    "    def __init__(self):\n",
    "\n",
    "        try:\n",
    "            self.fall_constraints = fall_constraints\n",
    "            self.input_params = input_params\n",
    "            self.model_params = model_params\n",
    "\n",
    "            self.detector_model, self.pose_model = self._init_model(**model_params)\n",
    "            self.conf_thresh = model_params[\"conf_thresh\"]\n",
    "            self.person_id = model_params[\"person_id\"]\n",
    "\n",
    "            self.video_name = input_params[\"video_name\"]\n",
    "            self.img_size = input_params[\"img_size\"]\n",
    "\n",
    "            self.min_non_dets = fall_constraints[\"min_non_dets\"]\n",
    "            self.box_size_thresh = fall_constraints[\"box_size_thresh\"]\n",
    "            self.anks_shdr_thresh = fall_constraints[\"anks_shdr_thresh\"]\n",
    "            self.dist_hist = fall_constraints[\"dist_hist\"]\n",
    "            self.dist_count = fall_constraints[\"dist_count\"]\n",
    "            self.fall_interval = fall_constraints[\"fall_interval\"]\n",
    "\n",
    "            self.verbose = fall_constraints[\"verbose\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}\".format(e))\n",
    "\n",
    "    def _init_model(self, **model_params):\n",
    "\n",
    "        detector_model = get_model(\n",
    "            model_params[\"detector_name\"], pretrained=model_params[\"pretrained\"]\n",
    "        )\n",
    "        pose_model = get_model(\n",
    "            model_params[\"pose_name\"], pretrained=model_params[\"pretrained\"]\n",
    "        )\n",
    "        detector_model.reset_class([\"person\"], reuse_weights=[\"person\"])\n",
    "\n",
    "        return detector_model, pose_model\n",
    "\n",
    "    def entry(self):\n",
    "\n",
    "        xpart_tracker, ypart_tracker = reset_tracker()  # Keypoint tracking list\n",
    "        (\n",
    "            frame_num,\n",
    "            frame_prev,\n",
    "            frame_curr,\n",
    "            zero_dets,\n",
    "        ) = reset_counts()  # Frame numbers tracking\n",
    "        fall_idx = -1\n",
    "\n",
    "        # Loading the video using opencv\n",
    "        cap = cv2.VideoCapture(self.video_name)\n",
    "        i = 0\n",
    "\n",
    "        # Looping through the video\n",
    "        while (cap.isOpened()) and i < video_stats[\"total_frames\"]:\n",
    "\n",
    "            (fetch, frame) = cap.read()\n",
    "            # Convert to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            i += 1\n",
    "\n",
    "            if i < 350 or i > 450:\n",
    "                continue\n",
    "\n",
    "            x, orig_img = preprocess(frame, self.img_size)\n",
    "            x, orig_img = mx.nd.array(x), mx.nd.array(orig_img)\n",
    "\n",
    "            # Detect person\n",
    "            box_ids, scores, bboxes = self.detector_model(x)\n",
    "            x_min, y_min, x_max, y_max = (\n",
    "                bboxes[0][0][0],\n",
    "                bboxes[0][0][1],\n",
    "                bboxes[0][0][2],\n",
    "                bboxes[0][0][3],\n",
    "            )\n",
    "            w, h = (x_max - x_min).asscalar(), (y_max - y_min).asscalar()\n",
    "\n",
    "            # Check for minimum person detection confidence and bbox dimension of person\n",
    "            if (\n",
    "                (scores[:, 0:1, :][0][0].asscalar() > self.conf_thresh)\n",
    "                and w > self.box_size_thresh[0]\n",
    "                and h > self.box_size_thresh[1]\n",
    "            ):\n",
    "\n",
    "                orig_img, box_ids, scores, bboxes = (\n",
    "                    to_np(orig_img),\n",
    "                    to_np(box_ids),\n",
    "                    to_np(scores),\n",
    "                    to_np(bboxes),\n",
    "                )\n",
    "\n",
    "                # get pose keypoints\n",
    "                try:\n",
    "                    # Crop the bbox area from detector output from original image, transform it for pose model\n",
    "                    pose_input, upscale_bbox = detector_to_simple_pose(\n",
    "                        orig_img,\n",
    "                        box_ids[:, 0:1, :],\n",
    "                        scores[:, 0:1, :],\n",
    "                        bboxes[:, 0:1, :],\n",
    "                        thr=self.conf_thresh,\n",
    "                        person_id=self.person_id,\n",
    "                    )\n",
    "\n",
    "                    pose_input, upscale_bbox = to_mx(pose_input), to_mx(upscale_bbox)\n",
    "\n",
    "                    predicted_heatmap = self.pose_model(pose_input)\n",
    "                    predicted_heatmap, upscale_bbox = to_np(predicted_heatmap), to_np(\n",
    "                        upscale_bbox\n",
    "                    )\n",
    "                    # process pose model output to get key point coordinates\n",
    "                    pred_coords, confidence = heatmap_to_coord(\n",
    "                        predicted_heatmap, upscale_bbox\n",
    "                    )\n",
    "\n",
    "                    # Append current prediction to previous\n",
    "                    xpart_tracker = update_x(pred_coords[0][:, 0], xpart_tracker)\n",
    "                    ypart_tracker = update_y(pred_coords[0][:, 1], ypart_tracker)\n",
    "\n",
    "                    result = fall_detection(\n",
    "                        ypart_tracker,\n",
    "                        self.anks_shdr_thresh,\n",
    "                        self.dist_hist,\n",
    "                        self.dist_count,\n",
    "                    )\n",
    "\n",
    "                    if result:\n",
    "                        # Flag next fall after fall_interval frames\n",
    "                        if fall_idx == -1 or (i - fall_idx) >= (self.fall_interval):\n",
    "                            time_stamp = time.strftime(\n",
    "                                \"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())\n",
    "                            )\n",
    "                            print(\n",
    "                                f\"Fall Detected at {time_stamp} {i}, saving image to logs.\"\n",
    "                            )\n",
    "                            fall_idx = i\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Exception: {}\".format(e))\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                # Reset tracker if no person is detected for more than `min_non_dets` continuous frames\n",
    "                if zero_dets > self.min_non_dets:\n",
    "                    xpart_tracker, ypart_tracker = reset_tracker()\n",
    "                    frame_num, frame_prev, frame_curr, zero_dets = reset_counts()\n",
    "                    continue\n",
    "                # Track consecutive non detections\n",
    "                frame_prev, frame_curr = frame_curr, frame_num\n",
    "                if frame_curr - frame_prev == 1:\n",
    "                    zero_dets += 1\n",
    "                else:\n",
    "                    zero_dets = 0\n",
    "\n",
    "            frame_num += 1\n",
    "\n",
    "            dist = round((ypart_tracker[\"anks\"][-1] - ypart_tracker[\"shdr\"][-1]), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"Fall Detected in frame : {}\".format(max(-1, fall_idx-1)),\n",
    "                (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"Ankle Shoulder Distance : {}\".format(dist),\n",
    "                (10, 75),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "            )\n",
    "            # Save the fall image\n",
    "            if fall_idx == i: plt.imsave(\"logs/fall.jpg\", frame)\n",
    "            \n",
    "            # Output the frame\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "\n",
    "            # Clear output\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # observe the keypress by the user\n",
    "            keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the user pressed \"q\", then stop looping\n",
    "            if keypress == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # free up memory\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    FallDetection().entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Publish to AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A lambda is already provided and ready for use in the Lambda folder (zip file)\n",
    "* Use this code snippet to upload and publish it to Lambda Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet uses boto3 to create an IAM role named LambdaBasicExecution with basic \n",
    "lambda execution permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\":[\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam_client = boto3.client(\"iam\")\n",
    "\n",
    "iam_client.create_role(\n",
    "    RoleName=\"FallDetectionExecutionRole\",\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zip -o ../Lambda/fall-detection.zip ../Lambda/fall_detector.py ../Lambda/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "lambda_client = boto3.client(\"lambda\")\n",
    "\n",
    "with open(\"../Lambda/fall-detection.zip\", \"rb\") as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "role = iam_client.get_role(RoleName=\"FallDetectionExecutionRole\")\n",
    "response_create_function = lambda_client.create_function(\n",
    "    FunctionName=\"FallDetectionLambda\",\n",
    "    Runtime=\"python3.7\",\n",
    "    Role=role[\"Role\"][\"Arn\"],\n",
    "    Handler=\"fall_detector.main()\",\n",
    "    Code=dict(ZipFile=zipped_code),\n",
    "    Timeout=120,  # Maximum allowable timeout\n",
    "    MemorySize=2048,\n",
    ")\n",
    "\n",
    "\n",
    "# Publish Lambda\n",
    "response = lambda_client.publish_version(FunctionName=\"FallDetectionLambdaDemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an ARN?** : Amazon Resource Names (ARNs) uniquely identify AWS resources.\n",
    "\n",
    "The following Python snippet will publish the Lambda Function created above, and return an ARN with a version. \n",
    "\n",
    "This version arn can be used to go directly to the Panorama console and deploy this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.publish_version(FunctionName=\"FallDetectionLambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the details of the lambda function that was just published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    \"\"\"\n",
    "    Helper Function for Fomatting Output\n",
    "    \"\"\"\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    \n",
    "    \n",
    "function_arn = response[\"FunctionArn\"]\n",
    "function_arn_version = list(response[\"FunctionArn\"].split(\":\"))[-1]\n",
    "lambda_url = (\n",
    "    \"https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/\"\n",
    "    + response[\"FunctionName\"]\n",
    "    + \"/versions/\"\n",
    "    + response[\"Version\"]\n",
    "    + \"?tab=configuration\"\n",
    ")\n",
    "\n",
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_model_to_s3(model, bucket = 'aws-panorama-models-bucket'):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    \n",
    "    key = '../../Models/' + model\n",
    "    \n",
    "    s3.Object(bucket, model).put(Body=open(key, 'rb'))\n",
    "    \n",
    "    bucket_name = bucket\n",
    "    \n",
    "    \n",
    "    location = boto3.client('s3').get_bucket_location(Bucket='aws-panorama-models-bucket')['LocationConstraint']\n",
    "    url = \"s3://{}/{}\".format(bucket_name, model)\n",
    "    \n",
    "    printmd(\"**S3 Path** : **{}**\".format(url), color=\"black\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'simple_pose_resnet50_v1d.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_model_to_s3(model = 'ssd_512_resnet50_v1_voc.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy the Application\n",
    "\n",
    "The Lambda is now created and published. You are now ready to deploy your model and the published lambda function, to the Panorama device\n",
    "\n",
    "The instructions to deploy are linked below\n",
    "\n",
    "[Creating Application Instructions Here](https://docs.aws.amazon.com/panorama/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some useful information about Lambda and the model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Function Arn** : **{}**\".format(function_arn), color=\"black\")\n",
    "printmd(\"**Function Arn Version** : **{}**\".format(function_arn_version), color=\"black\")\n",
    "printmd(\"**Lambda Console Link** : **{}**\".format(lambda_url), color=\"black\")\n",
    "\n",
    "printmd(\"**Inputs for SSD Model**\", color=\"black\")\n",
    "print('     ')\n",
    "printmd(\"**Input Name** : **{}**\".format('data'), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format([1,3,512,512]), color=\"black\")\n",
    "\n",
    "printmd(\"**Inputs for Simple Pose model**\", color=\"black\")\n",
    "print('     ')\n",
    "printmd(\"**Input Name** : **{}**\".format('data'), color=\"black\")\n",
    "printmd(\"**Shape** : **{}**\".format([1, 3, 256, 192]), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extending the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The choice of keypoints is subjective. \n",
    "- One can choose those body-parts that appear consistenly over video (for high confidence, less noisy prediction)  and measure distance accordingly.\n",
    "- Choose the fall detection thresholds based on the dataset and camera angle orientation.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
